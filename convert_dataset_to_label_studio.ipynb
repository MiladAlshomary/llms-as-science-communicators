{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d78efec-68b0-47f6-8d88-af0cd01c16fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('./src-py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d24b5e38-93dc-4098-97de-bb786a750a92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8267ab4-7483-4e08-aae9-2c199005edd1",
   "metadata": {},
   "source": [
    "#### Write Coversations to Label-studio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7839465b-7633-4ec2-922f-99ffd6ccdaef",
   "metadata": {},
   "source": [
    "This shows the scores of ChatGPT and the reasoning in the label-studio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d86b3529-2a1f-4bef-a360-d26a65127ba3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def write_dialogues_to_label_studio_format(eval_results, model_name, output_path):\n",
    "    output = []\n",
    "    for item in eval_results:\n",
    "        dlg = [{'text': turn['content'], 'author': 'Journalist' if turn['role']=='assistant' else 'Researcher'} for turn in item['generated_conversation'][2:]]\n",
    "        obj = {\n",
    "            'prompt_name': '',\n",
    "            'sc_title' : item['paper_id'],\n",
    "            'sc_abstract':item['generated_conversation'][1]['content'],\n",
    "            'dialogue' : dlg,\n",
    "            'llm_eval'  : '\\n\\n'.join(['{}: {} \\n {}'.format(aspect, item['{}_eval_prompt_scoring_parsed'.format(aspect)]['score'],\n",
    "                                                             item['{}_eval_prompt_scoring_parsed'.format(aspect)]['reasons']) \n",
    "                                       for aspect in ['scientific', 'societal', 'clarity']]),\n",
    "            'llam_societal_impact_eval': 'Score: {}\\nReason: {}'.format(item['societal_eval_prompt_scoring_parsed']['score'], \n",
    "                                                                       item['societal_eval_prompt_scoring_parsed']['reasons']\n",
    "                                                                ),\n",
    "            'llam_scientific_context_eval': 'Score: {}\\nReason {}'.format(item['scientific_eval_prompt_scoring_parsed']['score'], \n",
    "                                                                       item['scientific_eval_prompt_scoring_parsed']['reasons']\n",
    "                                                                ),\n",
    "            'llam_clarity_eval': 'Score: {}\\nReason {}'.format(item['clarity_eval_prompt_scoring_parsed']['score'], \n",
    "                                                                       item['clarity_eval_prompt_scoring_parsed']['reasons']\n",
    "                                                                ) ,\n",
    "            'gen-model': model_name\n",
    "        }\n",
    "\n",
    "        #obj['dialogue'] = [x for turn in obj['dialogue'] for x in turn]\n",
    "        \n",
    "        for aspect in ['scientific', 'societal', 'clarity']:\n",
    "            obj[aspect] = item['{}_eval_prompt_scoring_parsed'.format(aspect)]['score']\n",
    "        output.append(obj)\n",
    "    \n",
    "    json.dump(output, open(output_path, 'w'))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6262e62-e523-4ee4-b461-7537fd431225",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = '/mnt/swordfish-pool2/milad/communicating-science-to-the-public'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c6d4265-3eaf-40f2-bd4c-2ae3ab460fd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "baseline_llama3_evaluated_ds = datasets.load_from_disk(ds_path + '/baseline-advanced-prompt-llama3-test-conv-ds/ds_eval')\n",
    "#ft_llama3_evaluated_ds = datasets.load_from_disk(ds_path + '/ft-40k-llama3-test-conv-ds/ds_eval')\n",
    "baseline_qwen_evaluated_ds = datasets.load_from_disk(ds_path + '/baseline-qwen-test-conv-ds/ds_eval')\n",
    "#ft_qwen_evaluated_ds = datasets.load_from_disk(ds_path + '/ft-40k-qwen-test-conv-ds/ds_eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c08f79a-fc38-408f-bc75-d92c96eb418a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['paper_id', 'paper_title', 'paper_text', 'prompt', 'completion', '__index_level_0__', 'pr-article', 'generated_conversation', 'conversation', 'clarity_eval_prompt_scoring_parsed', 'scientific_eval_prompt_scoring_parsed', 'societal_eval_prompt_scoring_parsed'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_qwen_evaluated_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89bd1620-3eee-46e0-bdc4-d45ee1aa89b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "json_output  = write_dialogues_to_label_studio_format(baseline_llama3_evaluated_ds, 'baseline-llama3', './data/llama3_baseline_label_studio_tasks.json')\n",
    "#json_output  = write_dialogues_to_label_studio_format(ft_llama3_evaluated_ds, 'ft-llama3', './data/llama3_ft_label_studio_tasks.json')\n",
    "json_output  = write_dialogues_to_label_studio_format(baseline_qwen_evaluated_ds, 'baseline-qwen', './data/qwen_baseline_label_studio_tasks.json')\n",
    "#json_output  = write_dialogues_to_label_studio_format(ft_qwen_evaluated_ds, 'ft-qwen', './data/qwen_ft_label_studio_tasks.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8e434e-ed3b-4939-9805-9eb6df6489c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### This will compare chat-gpt ranking with human ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "36908749-bf3a-4d75-8e64-2a0588027614",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all_prompts = get_prompt_compositions()\n",
    "\n",
    "# gpt3_eval_results = get_prompts_conversations('/mnt/swordfish-pool2/milad/communicating-science-to-the-public/gpt3-gen-conv/', 100, all_prompts, w_eval_res=True)\n",
    "# llama3_eval_results = get_prompts_conversations('/mnt/swordfish-pool2/milad/communicating-science-to-the-public/llama3-gen-conv/', 100, all_prompts, w_eval_res=True)\n",
    "\n",
    "# gpt3_eval_results['composite-na-generic-guidelines'] = gpt3_eval_results['composite-na-generic-guidelines'].add_column('gen-source', ['gpt3-generic-guidelines']* 50)\n",
    "# gpt3_eval_results['composite-na-pr-guided']          = gpt3_eval_results['composite-na-pr-guided'].add_column('gen-source', ['gpt3-pr-guided']* 50)\n",
    "# llama3_eval_results['composite-na-generic-guidelines'] = llama3_eval_results['composite-na-generic-guidelines'].add_column('gen-source', ['llama3-generic-guidelines']* 49)\n",
    "# llama3_eval_results['composite-na-pr-guided'] = llama3_eval_results['composite-na-pr-guided'].add_column('gen-source', ['llama3-pr-guided']* 50)\n",
    "\n",
    "# dataset_df = datasets.concatenate_datasets([gpt3_eval_results['composite-na-generic-guidelines'], gpt3_eval_results['composite-na-pr-guided'], llama3_eval_results['composite-na-generic-guidelines'], llama3_eval_results['composite-na-pr-guided']]).to_pandas()\n",
    "\n",
    "# dataset_df.to_json('/mnt/swordfish-pool2/milad/communicating-science-to-the-public/all_sample_generated_conversations_with_eval_100.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632d4877-3e9d-4910-aa1d-b5b2b8bdc934",
   "metadata": {},
   "source": [
    "Now, we have two datasets with evalaution '/mnt/swordfish-pool2/milad/communicating-science-to-the-public/all_sample_generated_conversations_with_eval.json' old and the new one '/mnt/swordfish-pool2/milad/communicating-science-to-the-public/all_sample_generated_conversations_with_eval_100.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b3404a-78f0-421c-8375-86272f7c3f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.read_json('/mnt/swordfish-pool2/milad/communicating-science-to-the-public/all_sample_generated_conversations_with_eval_100.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0956ac2e-5b8f-44f5-87d0-d0aef9409e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df[['Topic']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0085f23-96b8-4a06-bd7b-27cb92c66693",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_label_studio_eval(dataset_df, evaluation_aspect):\n",
    "    dataset_df['score'] = dataset_df[evaluation_aspect  + '_eval_prompt_scoring_parsed'].apply(lambda x: x['score'])\n",
    "    dataset_df['reasons']= dataset_df[evaluation_aspect + '_eval_prompt_scoring_parsed'].apply(lambda x: str(x['score']) + ' : ' + str(x['reasons']))\n",
    "    \n",
    "    dataset_df_grouped = dataset_df.groupby(['pr-title']).agg({\n",
    "        'conversation': list,\n",
    "        'score': list,\n",
    "        'reasons': list,\n",
    "        'gen-source': list,\n",
    "        'Topic': lambda x: list(x)[0],\n",
    "    }).reset_index()\n",
    "    \n",
    "    dataset_df_grouped['conversation_comparison'] = dataset_df_grouped.apply(lambda row: sorted(list(zip(row['conversation'], row['gen-source'], row['reasons'], row['score'])), key=lambda x: -x[3]), axis=1)\n",
    "    \n",
    "    #creating pairs for comparison\n",
    "    pairs_ds = []\n",
    "    for idx, row in dataset_df_grouped.iterrows():\n",
    "        ranked_conv = row['conversation_comparison']\n",
    "        if ranked_conv[0][-1] > ranked_conv[-1][-1]:\n",
    "            dlg1 = [utter.split(':') for utter in ranked_conv[0][0].split('\\n\\n') if 'Journalist' in utter or 'Researcher' in utter]\n",
    "            dlg1 = [utter for utter in dlg1 if len(utter) > 1]\n",
    "            dlg1 = [{'text': utter[1].replace(\"**\", \"\"), 'author': utter[0].replace(\"**\", \"\")} for utter in dlg1]\n",
    "\n",
    "            dlg2 = [utter.split(':') for utter in ranked_conv[-1][0].split('\\n\\n') if 'Journalist' in utter or 'Researcher' in utter]\n",
    "            dlg2 = [utter for utter in dlg2 if len(utter) > 1]\n",
    "            dlg2 = [{'text': utter[1].replace(\"**\", \"\"), 'author': utter[0].replace(\"**\", \"\")} for utter in dlg2]\n",
    "            \n",
    "            json_obj = {\n",
    "                'item_1_conv'  : dlg1,\n",
    "                'item_1_source': ranked_conv[0][1],\n",
    "                'item_1_scoring' : ranked_conv[0][2],\n",
    "                'item_2_conv'  : dlg2,\n",
    "                'item_2_source': ranked_conv[-1][1],\n",
    "                'item_2_scoring' : ranked_conv[-1][2],\n",
    "                'pr_title' : row['pr-title'],\n",
    "                'topic': row['Topic'],\n",
    "                'eval_aspect': evaluation_aspect\n",
    "            }\n",
    "\n",
    "            instance = dataset_df[dataset_df['pr-title'] == row['pr-title']]\n",
    "            #print(instance)\n",
    "            json_obj['sc_abstract'] = instance['sc-intro'].tolist()[0]\n",
    "            json_obj['pr_article']  = instance['pr-summary'].tolist()[0] + '\\n=======\\n\\n' + instance['pr-article'].tolist()[0]\n",
    "            pairs_ds.append(json_obj)\n",
    "    return pairs_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fe26e848-4270-432e-83b3-bd2f072eeec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(generate_label_studio_eval(dataset_df, evaluation_aspect='faithfull'), open('/mnt/swordfish-pool2/milad/communicating-science-to-the-public/evaluate-gpt-4-faithfull_new.json', 'w'))\n",
    "json.dump(generate_label_studio_eval(dataset_df, evaluation_aspect='scientific'), open('/mnt/swordfish-pool2/milad/communicating-science-to-the-public/evaluate-gpt-4-scientific_new.json', 'w'))\n",
    "json.dump(generate_label_studio_eval(dataset_df, evaluation_aspect='societal'), open('/mnt/swordfish-pool2/milad/communicating-science-to-the-public/evaluate-gpt-4-societal_new.json', 'w'))\n",
    "json.dump(generate_label_studio_eval(dataset_df, evaluation_aspect='relevancy'), open('/mnt/swordfish-pool2/milad/communicating-science-to-the-public/evaluate-gpt-4-relevancy_new.json', 'w'))\n",
    "json.dump(generate_label_studio_eval(dataset_df, evaluation_aspect='clarity'), open('/mnt/swordfish-pool2/milad/communicating-science-to-the-public/evaluate-gpt-4-clarity_new.json', 'w'))\n",
    "json.dump(generate_label_studio_eval(dataset_df, evaluation_aspect='factuality'), open('/mnt/swordfish-pool2/milad/communicating-science-to-the-public/evaluate-gpt-4-factuality_new.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e8c0c433-dc8c-4af7-ace9-30918992be0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "json.dump(generate_label_studio_eval(dataset_df, evaluation_aspect='faithfull'), open('/mnt/swordfish-pool2/milad/communicating-science-to-the-public/evaluate-gpt-4-faithfull.json', 'w'))\n",
    "json.dump(generate_label_studio_eval(dataset_df, evaluation_aspect='scientific'), open('/mnt/swordfish-pool2/milad/communicating-science-to-the-public/evaluate-gpt-4-scientific.json', 'w'))\n",
    "json.dump(generate_label_studio_eval(dataset_df, evaluation_aspect='societal'), open('/mnt/swordfish-pool2/milad/communicating-science-to-the-public/evaluate-gpt-4-societal.json', 'w'))\n",
    "json.dump(generate_label_studio_eval(dataset_df, evaluation_aspect='relevancy'), open('/mnt/swordfish-pool2/milad/communicating-science-to-the-public/evaluate-gpt-4-relevancy.json', 'w'))\n",
    "json.dump(generate_label_studio_eval(dataset_df, evaluation_aspect='clarity'), open('/mnt/swordfish-pool2/milad/communicating-science-to-the-public/evaluate-gpt-4-clarity.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f114a301-6e5c-4634-9c26-01d2d15d54e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_output  = write_dialogues_to_label_studio_format(ft_llama3_evaluated_ds, 'ft-llama3', './data/ft_llama3_for_label_studio.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
