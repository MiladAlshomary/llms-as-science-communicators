{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "614ef48b-14f0-4de1-a243-6b961927a35c",
   "metadata": {},
   "source": [
    "### Baseline models to consider:\n",
    " - BART model from the work of SciTech News\n",
    " - Zero and Few-shot prompting of LLAMA-3\n",
    " - Fine-tuned LLAMA-3\n",
    " - BART model extended with conversation between (non) fine-tuned LLAMA-3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a2276a5-e797-43f2-96d8-ce46e0a07590",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b596b939-bf57-4b15-91d4-e3be81dd54ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44c06329-6589-4132-990c-ec9d1f9119ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TRANSFORMERS_CACHE'] = '/mnt/swordfish-pool2/milad/hf-cache'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/mnt/swordfish-pool2/milad/hf-cache'\n",
    "os.environ['OPENAI_API_KEY'] = 'zTbZNk16Ik1pZnqLn38ZT3BlbkFJImq3pd7widkr7RzsC771'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "sys.path.append('./src-py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "271bc6e9-d024-41ea-bbf6-9695121cecd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/swordfish-pool2/milad/conda-envs/datadreamer/lib/python3.12/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import datasets\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tabulate import tabulate\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, pipeline\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dca823a-e5b5-4c79-965e-8bf9d943c43f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import utils\n",
    "import prompts\n",
    "import datadreamer_generation\n",
    "from llm_based_evaluation import *\n",
    "from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6dfdbc1-1416-4d71-9104-55a3bcf0f56c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds_path = '/mnt/swordfish-pool2/milad/communicating-science-to-the-public/eval_experiment_500/baseline_llama3_gen_conv/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36134e5f-4d62-4260-8c2e-4d4f7a1c8cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test = datasets.load_from_disk(ds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f0f83c3-421b-47a5-a402-c09610837906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['paper_id', 'paper_title', 'paper_text', 'prompt', 'completion', 'pr-article', 'topic', '__index_level_0__', 'generated_conversation', 'conversation'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431e0dbe-c81b-438e-a30a-7918717f64a9",
   "metadata": {},
   "source": [
    "### Zero and few shot journalistic summarization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31e7d5ef-6a67-4903-82ba-b4314fc94ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datadreamer.llms import HFTransformers, ParallelLLM, OpenAI\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "q=BitsAndBytesConfig(load_in_8bit=True)\n",
    "llama3 = HFTransformers(\"meta-llama/Meta-Llama-3-8B-Instruct\", device='auto', quantization_config=q, dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b644e46-237d-444d-bb59-b2ccbb599960",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test = datasets.load_from_disk(ds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b67f1ccc-72f4-4bac-b156-ed38e17b8c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = '/mnt/swordfish-pool2/milad/communicating-science-to-the-public/generated-press-releases/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26cfd152-13f6-4aa0-aff3-407557f1bfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mbaseline_pr_generation\u001b[0m/  \u001b[01;34mbaseline_pr_generation_final_ds\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls /mnt/swordfish-pool2/milad/communicating-science-to-the-public/generated-press-releases/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f27d615-a91a-4670-9ce4-d0b96a6bc485",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Initialized. 🚀 Dreaming to folder: /mnt/swordfish-pool2/milad/communicating-science-to-the-public/generated-press-releases/baseline_pr_generation\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'original_ds' results loaded from disk. 🙌 It was previously run and saved.\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'original_ds (map)' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'original_ds (map)' will run lazily. 🥱\n",
      "/local/nlp/milad/code/llms-as-science-communicators/src-py/datadreamer_generation.py:332: UserWarning: You did not specify `total_num_rows`, so we cannot automatically update the progress % for this step. Either specify map(..., total_num_rows=#) or, to disable this warning, specify map(.., auto_progress = False)\n",
      "  datasource = datasource.map(lambda row: {'inputs_truncated': truncate_text(encoding, row['inputs'], max_input_tokens)})\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'original_ds (map) (map)' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'original_ds (map) (map)' will run lazily. 🥱\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' is running. ⏳\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Please write a press release article to communicate the science presented in the following scientific paper. Your output should be:\n",
      "        \"Press Release Article\": \"The press release article about the paper\"\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'original_ds (map)' finished running lazily. 🎉\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'original_ds (map) (map)' finished running lazily. 🎉\n",
      "/mnt/swordfish-pool2/milad/conda-envs/datadreamer/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 30 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 40 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 50 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 60 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 70 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 80 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 90 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 100 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 110 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 120 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 130 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 140 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 150 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 160 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 170 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 180 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 190 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 200 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 210 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 220 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 230 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 240 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 250 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 260 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 270 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 280 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 290 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 300 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 310 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 320 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 330 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 340 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 350 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 360 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 370 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 380 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 390 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 400 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 410 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 420 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 430 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 440 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 450 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 460 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 470 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 480 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 490 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 500 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' finished and is saved to disk. 🎉\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings (select_columns)' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings (select_columns)' finished running lazily. 🎉\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'zipped(original_ds (map) (map), pr-writings (select_columns))' is running. ⏳\n",
      "/mnt/swordfish-pool2/milad/conda-envs/datadreamer/lib/python3.12/site-packages/datadreamer/steps/step_operations.py:383: UserWarning: You did not specify `total_num_rows`, so we cannot automatically update the progress % for this step. Either specify LazyRows(..., total_num_rows=#) or, to disable this warning, specify LazyRows(.., auto_progress = False)\n",
      "  return LazyRows(\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'zipped(original_ds (map) (map), pr-writings (select_columns))' will run lazily. 🥱\n",
      "/local/nlp/milad/code/llms-as-science-communicators/src-py/datadreamer_generation.py:350: UserWarning: You did not specify `total_num_rows`, so we cannot automatically update the progress % for this step. Either specify map(..., total_num_rows=#) or, to disable this warning, specify map(.., auto_progress = False)\n",
      "  zipped_step = zipped_step.map(lambda row: parse_pr_article(row))\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'zipped(original_ds (map) (map), pr-writings (select_columns)) (map)' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'zipped(original_ds (map) (map), pr-writings (select_columns)) (map)' will run lazily. 🥱\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to find PR Article in the output\n",
      "Failed to find PR Article in the output\n",
      "Failed to find PR Article in the output\n",
      "Failed to find PR Article in the output\n",
      "Failed to find PR Article in the output\n",
      "Failed to find PR Article in the output\n",
      "Failed to find PR Article in the output\n",
      "Failed to find PR Article in the output\n",
      "Failed to find PR Article in the output\n",
      "Failed to find PR Article in the output\n",
      "Failed to find PR Article in the output\n",
      "Failed to find PR Article in the output\n",
      "Failed to find PR Article in the output\n",
      "Failed to find PR Article in the output\n",
      "Failed to find PR Article in the output\n",
      "Failed to find PR Article in the output\n",
      "Failed to find PR Article in the output\n",
      "Failed to find PR Article in the output\n",
      "Failed to find PR Article in the output\n",
      "Failed to find PR Article in the output\n",
      "Failed to find PR Article in the output\n",
      "Failed to find PR Article in the output\n",
      "Failed to find PR Article in the output\n",
      "Failed to find PR Article in the output\n",
      "Failed to find PR Article in the output\n",
      "Failed to find PR Article in the output\n",
      "Failed to find PR Article in the output\n",
      "Failed to find PR Article in the output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'zipped(original_ds (map) (map), pr-writings (select_columns))' finished running lazily. 🎉\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'zipped(original_ds (map) (map), pr-writings (select_columns)) (map)' finished running lazily. 🎉\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Done. ✨ Results in folder: /mnt/swordfish-pool2/milad/communicating-science-to-the-public/generated-press-releases/baseline_pr_generation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029a69a971484399b6b6c4f23edb7790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = prompts.baseline_pr_generation\n",
    "prompt['inputs']['Scientific paper'] = 'paper_text'\n",
    "baseline_pr_generation_res = datadreamer_generation.generate_pr_articles(llama3, sample_test, ds_path, \n",
    "                                                                         tokenizer, prompt, max_input_tokens=1200, hub_name=None)\n",
    "baseline_pr_generation_res.save_to_disk(ds_path + '/'  + prompt['strategy_name'] + '_final_ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d366bacf-de66-4956-aab4-8ea3879fcc4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Initialized. 🚀 Dreaming to folder: /mnt/swordfish-pool2/milad/communicating-science-to-the-public/generated-press-releases/baseline_pr_generation_cot\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'original_ds' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'original_ds' finished and is saved to disk. 🎉\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'original_ds (map)' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'original_ds (map)' will run lazily. 🥱\n",
      "/local/nlp/milad/code/llms-as-science-communicators/src-py/datadreamer_generation.py:332: UserWarning: You did not specify `total_num_rows`, so we cannot automatically update the progress % for this step. Either specify map(..., total_num_rows=#) or, to disable this warning, specify map(.., auto_progress = False)\n",
      "  datasource = datasource.map(lambda row: {'inputs_truncated': truncate_text(encoding, row['inputs'], max_input_tokens)})\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'original_ds (map) (map)' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'original_ds (map) (map)' will run lazily. 🥱\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' is running. ⏳\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Please write a press release article to communicate the science presented in the following paper.\n",
      "        Before generating the press release, think step by step about the social impact of the research paper, the innovative aspects of the paper and how it is different from other research on the same topic, and how to communicate the problem, the approach and the results of the paper in a simple and accessible lanuage. Finally output the press release in the following format:\n",
      "        \"Press Release Article\": \"The press release article about the paper\"\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'original_ds (map)' finished running lazily. 🎉\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'original_ds (map) (map)' finished running lazily. 🎉\n",
      "/mnt/swordfish-pool2/milad/conda-envs/datadreamer/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 10 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 20 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 30 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 40 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 50 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 60 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 70 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 80 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 90 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 100 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 110 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 120 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 130 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 140 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 150 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 160 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 170 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 180 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 190 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 200 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 210 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 220 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 230 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 240 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 250 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 260 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 270 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 280 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 290 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 300 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 310 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 320 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 330 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 340 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 350 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 360 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 370 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 380 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 390 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 400 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 410 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 420 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 430 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 440 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 450 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 460 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 470 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 480 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 490 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' progress: 500 row(s) 🔄\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' finished and is saved to disk. 🎉\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings (select_columns)' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings (select_columns)' finished running lazily. 🎉\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'zipped(original_ds (map) (map), pr-writings (select_columns))' is running. ⏳\n",
      "/mnt/swordfish-pool2/milad/conda-envs/datadreamer/lib/python3.12/site-packages/datadreamer/steps/step_operations.py:383: UserWarning: You did not specify `total_num_rows`, so we cannot automatically update the progress % for this step. Either specify LazyRows(..., total_num_rows=#) or, to disable this warning, specify LazyRows(.., auto_progress = False)\n",
      "  return LazyRows(\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'zipped(original_ds (map) (map), pr-writings (select_columns))' will run lazily. 🥱\n",
      "/local/nlp/milad/code/llms-as-science-communicators/src-py/datadreamer_generation.py:350: UserWarning: You did not specify `total_num_rows`, so we cannot automatically update the progress % for this step. Either specify map(..., total_num_rows=#) or, to disable this warning, specify map(.., auto_progress = False)\n",
      "  zipped_step = zipped_step.map(lambda row: parse_pr_article(row))\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'zipped(original_ds (map) (map), pr-writings (select_columns)) (map)' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'zipped(original_ds (map) (map), pr-writings (select_columns)) (map)' will run lazily. 🥱\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to find PR Article in the output\n",
      "Failed to find PR Article in the output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'zipped(original_ds (map) (map), pr-writings (select_columns))' finished running lazily. 🎉\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'zipped(original_ds (map) (map), pr-writings (select_columns)) (map)' finished running lazily. 🎉\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Done. ✨ Results in folder: /mnt/swordfish-pool2/milad/communicating-science-to-the-public/generated-press-releases/baseline_pr_generation_cot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce05d9de51b465e8e10f2f5ab1398ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = prompts.baseline_pr_generation_cot\n",
    "prompt['inputs']['Scientific paper'] = 'paper_text'\n",
    "baseline_pr_generation_res = datadreamer_generation.generate_pr_articles(llama3, sample_test, ds_path, \n",
    "                                                                         tokenizer, prompt, max_input_tokens=1200, hub_name=None)\n",
    "baseline_pr_generation_res.save_to_disk(ds_path + '/'  + prompt['strategy_name'] + '_final_ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb83aa6a-ca30-4d88-9c37-229562f74fbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prompt = prompts.baseline_pr_generation_w_conv\n",
    "# prompt['inputs']['Scientific paper'] = 'paper_text'\n",
    "# baseline_pr_generation_res = datadreamer_generation.generate_pr_articles(llama3, sample_test, ds_path + '/generated-prs/'  + prompt['strategy_name'], \n",
    "#                                                                          tokenizer, prompt, max_input_tokens=1200, hub_name=None)\n",
    "# baseline_pr_generation_res.save_to_disk(ds_path + '/generated-prs/'  + prompt['strategy_name'] + '_final_ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73a83725-4d1d-4107-a4cf-70d0d84772ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee00cf07adbf4d359d31bc8fb362b51d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_test = sample_test.map(lambda row: {'generated-conversation-as-str': '\\n'.join(['{}: {}'.format('Journalist', x['content']) if x['role'] == 'assistant' else '{}: {}'.format('Researcher', x['content']) for x in row['generated_conversation'][1:]])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31970f1f-b54b-49f0-8b72-ea7c13c94c35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Initialized. 🚀 Dreaming to folder: /mnt/swordfish-pool2/milad/communicating-science-to-the-public/generated-press-releases/pr_generation_by_conv_summarization\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'original_ds' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'original_ds' finished and is saved to disk. 🎉\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'original_ds (map)' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'original_ds (map)' will run lazily. 🥱\n",
      "/local/nlp/milad/code/llms-as-science-communicators/src-py/datadreamer_generation.py:332: UserWarning: You did not specify `total_num_rows`, so we cannot automatically update the progress % for this step. Either specify map(..., total_num_rows=#) or, to disable this warning, specify map(.., auto_progress = False)\n",
      "  datasource = datasource.map(lambda row: {'inputs_truncated': truncate_text(encoding, row['inputs'], max_input_tokens)})\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'original_ds (map) (map)' is running. ⏳\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'original_ds (map) (map)' will run lazily. 🥱\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'pr-writings' is running. ⏳\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Please write a press release article to communicate the science presented in the following paper. The press-release should summarize the main points in the given conversation. The output should have the following format:\n",
      "        \"Press Release Article\": \"The press release article about the paper\"        \n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'original_ds (map)' finished running lazily. 🎉\n",
      "[ \u001b[35m🤖 Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m 💤 ] Step 'original_ds (map) (map)' finished running lazily. 🎉\n",
      "/mnt/swordfish-pool2/milad/conda-envs/datadreamer/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    }
   ],
   "source": [
    "prompt = prompts.pr_generation_by_conv_summarization\n",
    "prompt['inputs']['Conversation'] = 'generated-conversation-as-str'\n",
    "pr_generation_w_conv_res = datadreamer_generation.generate_pr_articles(llama3, sample_test, ds_path, tokenizer, prompt, hub_name=None)\n",
    "pr_generation_w_conv_res.save_to_disk(ds_path + prompt['strategy_name'] + '_final_ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90031709-7c27-4283-b8cb-6b882141cb55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Press Release Article:**\n",
      "\n",
      "**Unlocking the Secrets of Superconductors: Scientists Discover Hidden Patterns in Magnetic Materials**\n",
      "\n",
      "A team of scientists has made a groundbreaking discovery in the field of superconductors, uncovering hidden patterns in magnetic materials that could revolutionize our understanding of these complex systems.\n",
      "\n",
      "Researchers have long been fascinated by the phenomenon of superconductivity, where certain materials can conduct electricity with zero resistance at extremely low temperatures. However, the exact mechanisms behind this phenomenon have remained elusive, until now.\n",
      "\n",
      "In a new study, scientists have discovered that impurities in magnetic materials can create \"spin droplets\" - tiny regions of magnetic order that persist even when the material is cooled to near absolute zero. These droplets, which are invisible to the naked eye, can have a profound impact on the material's electronic properties, leading to the emergence of unconventional superconducting states.\n",
      "\n",
      "The research, published in a recent scientific paper, focused on the compound CeCoIn5, a rare material that is \"born\" as a quantum critical superconductor. By introducing small amounts of cadmium into the material, scientists were able to induce long-range magnetic order, which they then suppressed using applied pressure.\n",
      "\n",
      "Surprisingly, the team found that even when the global magnetic order was suppressed, the spin droplets persisted, creating a heterogeneous electronic state. This discovery has significant implications for our understanding of quantum criticality and the role of impurities in shaping the properties of correlated electron materials.\n",
      "\n",
      "\"This research opens up new avenues for exploring the properties of superconductors and other correlated materials,\" said Dr. [Name], lead author of the study. \"By understanding how impurities influence the behavior of these materials, we can develop new strategies for tuning their properties and unlocking their full potential.\"\n",
      "\n",
      "The study's findings have far-reaching implications for the development of new technologies, including high-temperature superconductors, which could revolutionize energy transmission and storage. The research also highlights the importance of considering the role of impurities in shaping the properties of complex materials, a crucial consideration for scientists working in this field.\n",
      "\n",
      "In conclusion, this groundbreaking study has shed new light on the mysteries of superconductors, revealing hidden patterns in magnetic materials that could have significant implications for our understanding of these complex systems. As scientists continue to explore the properties of correlated electron materials, this research serves as a powerful reminder of the importance of considering the role of impurities in shaping their behavior.\n",
      "++++++++++++++\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'baseline_pr_generation_cot_res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(baseline_pr_generation_res[clm][row])\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m++++++++++++++\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mbaseline_pr_generation_cot_res\u001b[49m[clm][row])\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m++++++++++++++\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(baseline_pr_generation_w_conv_res[clm][row])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'baseline_pr_generation_cot_res' is not defined"
     ]
    }
   ],
   "source": [
    "#clm='parsed-pr-article'\n",
    "clm='gen-pr'\n",
    "row=0\n",
    "print(baseline_pr_generation_res[clm][row])\n",
    "print('++++++++++++++')\n",
    "print(baseline_pr_generation_cot_res[clm][row])\n",
    "print('++++++++++++++')\n",
    "print(baseline_pr_generation_w_conv_res[clm][row])\n",
    "print('++++++++++++++')\n",
    "print(pr_generation_w_conv_res[clm][row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f73a3d12-93c6-48bd-9da8-5b92efbc1fb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the two trained llms\n",
    "# ds_path = '/mnt/swordfish-pool2/milad/communicating-science-to-the-public/generated-press-releases/llama3-trained-on-llama3-for-1-epochs/'\n",
    "\n",
    "# q=BitsAndBytesConfig(load_in_8bit=True)\n",
    "# llama3 = HFTransformers(\"meta-llama/Meta-Llama-3-8B-Instruct\", device='auto', adapter_name='miladalsh/llama3-trained-on-llama3-for-1-epochs', \n",
    "#                         quantization_config=q, dtype=torch.bfloat16)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# prompt = prompts.baseline_pr_generation_w_conv\n",
    "# prompt['inputs']['Scientific paper'] = 'sc-intro'\n",
    "# pr_generation_w_conv_res = baselines.generate_pr_articles(llama3, sample_test, ds_path, tokenizer, prompt, hub_name=None)\n",
    "# pr_generation_w_conv_res.save_to_disk(ds_path + prompt['strategy_name'] + 'final_ds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c572ce1-b778-4236-ae84-69357351a313",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9e53e8d-397e-43ab-900b-709c834f0fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = '/mnt/swordfish-pool2/milad/communicating-science-to-the-public/generated-press-releases/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6db1d4cc-4a22-4d45-9f25-846a972a7bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mbaseline_pr_generation\u001b[0m/\n",
      "\u001b[01;34mbaseline_pr_generation_cot\u001b[0m/\n",
      "\u001b[01;34mbaseline_pr_generation_cot_final_ds\u001b[0m/\n",
      "\u001b[01;34mbaseline_pr_generation_final_ds\u001b[0m/\n",
      "\u001b[01;34mpr_generation_by_conv_summarization\u001b[0m/\n",
      "\u001b[01;34mpr_generation_by_conv_summarization_final_ds\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls /mnt/swordfish-pool2/milad/communicating-science-to-the-public/generated-press-releases/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c740373e-d9c7-41a8-8563-9ed141fa37f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_pr_generation_res = datasets.load_from_disk(ds_path + '/baseline_pr_generation_final_ds')\n",
    "baseline_pr_generation_cot_res = datasets.load_from_disk(ds_path + '/baseline_pr_generation_cot_final_ds')\n",
    "baseline_pr_generation_conv_res = datasets.load_from_disk(ds_path +  '/pr_generation_by_conv_summarization_final_ds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6272993-fa3f-41b4-ba60-85a400814b03",
   "metadata": {},
   "source": [
    "#### Basic Eval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac5186d5-fc2a-4aa8-992f-46f83da8f0c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8eecdc208a044c89e95a7c824cfa4b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6adfe68966e4af8a25dbae78fcb840a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1202e2de9244f568db3be77e24fd8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7498565bb345cf9e5a510330396330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20276ef887c14a2c82d7eee18013848f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9219ca1dad34416e8a52584277bd6d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_results = {}\n",
    "\n",
    "eval_results['baseline_pr_generation'] = utils.evaluate_text_similarity(baseline_pr_generation_res['parsed-pr-article'],  baseline_pr_generation_res['pr-article'])\n",
    "eval_results['baseline_pr_generation_cot'] = utils.evaluate_text_similarity(baseline_pr_generation_cot_res['parsed-pr-article'],  baseline_pr_generation_cot_res['pr-article'])\n",
    "eval_results['pr_generation_w_conv'] = utils.evaluate_text_similarity(baseline_pr_generation_conv_res['parsed-pr-article'],  baseline_pr_generation_conv_res['pr-article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9ebf5736-adf9-4380-b623-83bd160416ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#baseline_pr_generation_cot_res['gen-pr'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7294db6-9bc6-48f1-a853-1878a56f31da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#baseline_pr_generation_cot_res['parsed-pr-article'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a506746d-b3fa-4f84-88bd-ad21fa3a63a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model                         Rouge-1    Rouge-L    BERT-f1\n",
      "--------------------------  ---------  ---------  ---------\n",
      "baseline_pr_generation          0.43       0.178      0.838\n",
      "baseline_pr_generation_cot      0.432      0.177      0.837\n",
      "pr_generation_w_conv            0.413      0.175      0.834\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(\n",
    "    [[name] + list(eval_res.values())[:3] for name, eval_res in eval_results.items()],\n",
    "    headers=['Model', 'Rouge-1', 'Rouge-L', 'BERT-f1']\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63d6492b-d590-468b-a806-f8ee2d83b286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model                            Rouge-1    Rouge-L    BERT-f1\n",
      "-----------------------------  ---------  ---------  ---------\n",
      "baseline_pr_generation             0.434      0.181      0.837\n",
      "baseline_pr_generation_cot         0.435      0.178      0.835\n",
      "baseline_pr_generation_w_conv      0.38       0.167      0.834\n",
      "pr_generation_w_conv               0.405      0.172      0.829\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(\n",
    "    [[name] + list(eval_res.values())[:3] for name, eval_res in eval_results.items()],\n",
    "    headers=['Model', 'Rouge-1', 'Rouge-L', 'BERT-f1']\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7e4a79-c663-4067-a0db-2a19de336e97",
   "metadata": {},
   "source": [
    "#### LLM-based evalaution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27bfbf96-53e6-4d4a-9fa8-56a84a0afd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grdound_truth_ds = baseline_pr_generation_res.map(lambda row: {'parsed-pr-article': row['pr-summary-and-article']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4c2a0fe1-635f-4cd0-839a-818a498be7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "press_release_datasets = {\n",
    "    'baseline_pr_generation' : [ datasets.load_from_disk(ds_path + '/baseline-generation/' + 'baseline_pr_generation_final_ds'), ds_path + '/baseline-generation/' + 'baseline_pr_generation_final_ds'],\n",
    "    'baseline_pr_generation_cot': [datasets.load_from_disk(ds_path + '/baseline-generation/' + 'baseline_pr_generation_cot_final_ds'),ds_path + '/baseline-generation/' + 'baseline_pr_generation_cot_final_ds'],\n",
    "    'baseline_pr_generation_w_conv': [datasets.load_from_disk(ds_path + '/baseline-generation/' + 'baseline_pr_generation_w_conv_final_ds'), ds_path + '/baseline-generation/' + 'baseline_pr_generation_w_conv_final_ds'],\n",
    "    'pr_generation_w_conv': [datasets.load_from_disk(ds_path + '/baseline-generation/' + 'pr_generation_by_conv_summarization_final_ds'), ds_path + '/baseline-generation/' + 'pr_generation_by_conv_summarization_final_ds'],\n",
    "    'gt_press_release' : [grdound_truth_ds, ds_path + '/gt-press-release'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9bd6ef-f35b-4891-949c-92723a7579a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /mnt/swordfish-pool2/milad/communicating-science-to-the-public/generated-press-releases//baseline-generation/baseline_pr_generation_final_ds from already saved file\n",
      "Loading /mnt/swordfish-pool2/milad/communicating-science-to-the-public/generated-press-releases//baseline-generation/baseline_pr_generation_cot_final_ds from already saved file\n",
      "Loading /mnt/swordfish-pool2/milad/communicating-science-to-the-public/generated-press-releases//baseline-generation/baseline_pr_generation_w_conv_final_ds from already saved file\n",
      "Loading /mnt/swordfish-pool2/milad/communicating-science-to-the-public/generated-press-releases//baseline-generation/pr_generation_by_conv_summarization_final_ds from already saved file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/494 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "prompts_to_eval = [prompts.pr_clarity_eval_prompt, prompts.pr_scientific_context_eval_prompt, prompts.pr_societal_context_eval_prompt]\n",
    "\n",
    "llm_eval_results = llm_based_evaluation(prompts_to_eval, press_release_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "71969268-86d8-4b57-b0a6-184616a82c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt                           Scientific Context    Social Context    Accessibility    Relevance\n",
      "-----------------------------  --------------------  ----------------  ---------------  -----------\n",
      "baseline_pr_generation                         1.74              1.95             3.6          2.43\n",
      "baseline_pr_generation_cot                     1.75              1.99             3.8          2.51\n",
      "baseline_pr_generation_w_conv                  1.7               2.04             3.86         2.53\n",
      "pr_generation_w_conv                           1.64              2.06             3.38         2.36\n",
      "gt_press_release                               2.48              2.22             4.39         3.03\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(\n",
    "        [[name] + get_llm_avg_scores(res) for name, res in llm_eval_results.items()],\n",
    "        headers=['Prompt', 'Scientific Context', 'Social Context', 'Accessibility', 'Relevance', 'Avg']\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
