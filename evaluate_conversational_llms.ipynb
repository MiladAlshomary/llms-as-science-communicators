{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a43c8d27-0b47-46ac-a3f7-e5a02134e727",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d78efec-68b0-47f6-8d88-af0cd01c16fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TRANSFORMERS_CACHE'] = '/mnt/swordfish-pool2/milad/hf-cache-new'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/mnt/swordfish-pool2/milad/hf-cache-new'\n",
    "os.environ[\"OPENAI_API_KEY\"]= 'xxx'\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '0'\n",
    "sys.path.append('./src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d24b5e38-93dc-4098-97de-bb786a750a92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma4608/.local/lib/python3.8/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "%autoreload\n",
    "import utils\n",
    "import prompts\n",
    "import random\n",
    "\n",
    "from tabulate import tabulate\n",
    "import tiktoken\n",
    "from transformers import AutoTokenizer\n",
    "from llm_based_evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d903fa8b-3c9e-4d0c-9a90-00c414488647",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8de285da-4e35-4041-8aa8-956e164feb64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(os.environ['hf_token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "846f3f93-4394-4627-be70-6bd557823095",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.llm_to_use='llama3'\n",
    "output_dir = '/mnt/swordfish-pool2/milad/communicating-science-to-the-public/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6a4d8a7-48f5-42a8-80f8-757e6a889e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0909cb91-afb0-4bf5-a5b1-17b1202ca5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_folder = \"/mnt/swordfish-pool2/milad/communicating-science-to-the-public/models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8866fa1b-c9d8-4a8d-91ba-723fc6c228d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6c1c877-9c93-42b5-9e22-9d399def61e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e2b8fb5c57407f99f707503fdd19e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/5188 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"miladalsh/sci-news\")\n",
    "training_ds = dataset['test'].filter(lambda row: row['source'] == 'SciNews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b10e2fa-ca27-4827-8065-cc8cf033dad0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f9db3a02394db1b4ae93afaefafdd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/4188 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be903fdaff94e1fb86601ca06e31da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4188 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc_intro = [utils.build_model_context(row, llama_tokenizer, max_token_number=3000) for i, row in training_ds.to_pandas().iterrows()]\n",
    "training_ds = training_ds.add_column('sc-intro', sc_intro)\n",
    "training_ds = training_ds.filter(lambda row: row['sc-intro'] != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f639112-42e3-4b00-b417-9718aaba03cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'pr-title', 'pr-article', 'pr-summary', 'sc-title', 'sc-article', 'sc-abstract', 'sc-section_names', 'sc-sections', 'sc-authors', 'source', 'Topic', 'Citation', 'Paper_URL', 'News_URL', 'pr-summary-and-article', 'sc-intro'],\n",
       "    num_rows: 4188\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a80dc623-f38a-4aba-93b1-d745c11a506d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5c9058f8704e8ea90f50731d8f9606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4188 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_ds.save_to_disk('/mnt/swordfish-pool2/milad/communicating-science-to-the-public/processed_test_ds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d647b70-c4cd-4831-a450-1a8d0b6ac1a6",
   "metadata": {},
   "source": [
    "### Evaluate Science Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38243aa0-986e-4b8f-acfd-40ab8416f2a8",
   "metadata": {},
   "source": [
    "- Now we will evalaute the following models on a sample from the test set using only the generic prompt\n",
    "    - LLAMA-3 baseline\n",
    "    - GPT-3 baseline\n",
    "    - LLAMA-3 fine-tuned on LLAMA-3 generated conversations\n",
    "    - LLAMA-3 fine-tuned on GPT-3 generated conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb1dff5-f182-452f-bbd9-ec3d969a4908",
   "metadata": {},
   "source": [
    "#### Load dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c6a6c9c-5662-4b27-a5da-0ef97605644b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 2156.76 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# test_ds = datasets.load_from_disk('/mnt/swordfish-pool2/milad/communicating-science-to-the-public/deepseek-final-conv-ds-preprocessed-test_journalist_ds')\n",
    "# test_df = pd.DataFrame(test_ds)\n",
    "# test_df = test_df.drop_duplicates('paper_id')\n",
    "# test_ds = datasets.Dataset.from_pandas(test_df)\n",
    "# sample_dataset = test_ds.select(range(500))\n",
    "# sample_dataset.save_to_disk('/mnt/swordfish-pool2/milad/communicating-science-to-the-public/processed_test_ds_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab677c22-2283-41a9-b6b2-c883bc625cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek_conv_dataset = datasets.load_from_disk('/mnt/swordfish-pool2/milad/communicating-science-to-the-public/deepseek-final-conv-ds-cleaned/')\n",
    "paper_id_to_article = {x['id']: x['pr-article'] for x in deepseek_conv_dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d7a45d0-6c54-47f4-9dae-eb4fa046e816",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_dataset = datasets.load_from_disk('/mnt/swordfish-pool2/milad/communicating-science-to-the-public/processed_test_ds_sample')\n",
    "sample_dataset = sample_dataset.map(lambda row: {'pr-article': paper_id_to_article[row['paper_id']]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cc9815d-741d-4b2e-81b8-92a79c356d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['paper_id', 'paper_title', 'paper_text', 'prompt', 'completion', '__index_level_0__', 'pr-article'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5773263-e456-41f3-9f8e-570ef6cfb874",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Generate baseline conversations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2725520-b9a5-4e7d-b748-a6a0a169c744",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prompts = utils.get_prompt_compositions()\n",
    "used_prompt = all_prompts[0]\n",
    "used_prompt['instruction'] = \"\"\"Please simulate a conversation between a researcher and a journalist regarding the researcher's scientific paper. The goal of the conversation is to gain a deeper understanding of the researcher's scientific paper and communicate its impact to the public through a journalistic report\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93c404ee-a808-4204-9d77-4416393103e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset= sample_dataset.rename_column('conversation', 'gt_conversation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce6a92c1-0d6d-41c6-81e6-a0669a6a73d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: /mnt/swordfish-pool2/milad/communicating-science-to-the-public/gpt3-test-conv-ds/composite-na-na\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'documents' was previously run and saved, but was outdated. ðŸ˜ž\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'documents' is running. â³\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'documents' finished and is saved to disk. ðŸŽ‰\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'documents (map)' is running. â³\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'documents (map)' will run lazily. ðŸ¥±\n",
      "/local/nlp/milad/code/communicating-science-to-the-public/src/datadreamer_generation.py:141: UserWarning: You did not specify `total_num_rows`, so we cannot automatically update the progress % for this step. Either specify map(..., total_num_rows=#) or, to disable this warning, specify map(.., auto_progress = False)\n",
      "  datasource = datasource.map(lambda row: {'inputs_truncated': truncate_text(encoding, row['inputs'], max_input_tokens)})\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'documents (map) (map)' is running. â³\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'documents (map) (map)' will run lazily. ðŸ¥±\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'generate conversations' was previously run and saved, but was outdated. ðŸ˜ž\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'generate conversations' is running. â³\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'documents (map)' finished running lazily. ðŸŽ‰\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'documents (map) (map)' finished running lazily. ðŸŽ‰\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'generate conversations' finished and is saved to disk. ðŸŽ‰\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'generate conversations (select_columns)' is running. â³\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'generate conversations (select_columns)' finished running lazily. ðŸŽ‰\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'zipped(documents (map) (map), generate conversations (select_columns))' is running. â³\n",
      "/mnt/swordfish-pool2/milad/conda-envs/datadreamer/lib/python3.12/site-packages/datadreamer/steps/step_operations.py:383: UserWarning: You did not specify `total_num_rows`, so we cannot automatically update the progress % for this step. Either specify LazyRows(..., total_num_rows=#) or, to disable this warning, specify LazyRows(.., auto_progress = False)\n",
      "  return LazyRows(\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'zipped(documents (map) (map), generate conversations (select_columns))' will run lazily. ðŸ¥±\n",
      "/local/nlp/milad/code/communicating-science-to-the-public/src/datadreamer_generation.py:160: UserWarning: You did not specify `total_num_rows`, so we cannot automatically update the progress % for this step. Either specify map(..., total_num_rows=#) or, to disable this warning, specify map(.., auto_progress = False)\n",
      "  zipped_step = zipped_step.map(lambda row: parse_conversation(row))\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'zipped(documents (map) (map), generate conversations (select_columns)) (map)' is running. â³\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'zipped(documents (map) (map), generate conversations (select_columns)) (map)' will run lazily. ðŸ¥±\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'zipped(documents (map) (map), generate conversations (select_columns))' finished running lazily. ðŸŽ‰\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'zipped(documents (map) (map), generate conversations (select_columns)) (map)' finished running lazily. ðŸŽ‰\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: /mnt/swordfish-pool2/milad/communicating-science-to-the-public/gpt3-test-conv-ds/composite-na-na\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1afc96e5e6545bab00fc9b1eb85d072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_dir = '/mnt/swordfish-pool2/milad/communicating-science-to-the-public/gpt3-test-conv-ds/'\n",
    "resulted_ds = datadreamer_generation.generate_conversation(output_dir, 'gpt-3', sample_dataset, used_prompt, gpt_tokenizer, max_input_tokens=1200)\n",
    "resulted_ds.save_to_disk(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da09ce55-6ae0-4d20-841d-672149fc1d02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Initialized. ðŸš€ Dreaming to folder: /mnt/swordfish-pool2/milad/communicating-science-to-the-public/llama3-test-conv-ds/composite-na-na\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'documents' is running. â³\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'documents' finished and is saved to disk. ðŸŽ‰\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'documents (map)' is running. â³\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'documents (map)' will run lazily. ðŸ¥±\n",
      "/local/nlp/milad/code/communicating-science-to-the-public/src/datadreamer_generation.py:141: UserWarning: You did not specify `total_num_rows`, so we cannot automatically update the progress % for this step. Either specify map(..., total_num_rows=#) or, to disable this warning, specify map(.., auto_progress = False)\n",
      "  datasource = datasource.map(lambda row: {'inputs_truncated': truncate_text(encoding, row['inputs'], max_input_tokens)})\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'documents (map) (map)' is running. â³\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'documents (map) (map)' will run lazily. ðŸ¥±\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'generate conversations' is running. â³\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'documents (map)' finished running lazily. ðŸŽ‰\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'documents (map) (map)' finished running lazily. ðŸŽ‰\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'generate conversations' progress: 30 row(s) ðŸ”„\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'generate conversations' progress: 60 row(s) ðŸ”„\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'generate conversations' progress: 90 row(s) ðŸ”„\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'generate conversations' progress: 120 row(s) ðŸ”„\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'generate conversations' progress: 150 row(s) ðŸ”„\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'generate conversations' progress: 180 row(s) ðŸ”„\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'generate conversations' progress: 210 row(s) ðŸ”„\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'generate conversations' progress: 240 row(s) ðŸ”„\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'generate conversations' progress: 270 row(s) ðŸ”„\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'generate conversations' progress: 300 row(s) ðŸ”„\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'generate conversations' progress: 330 row(s) ðŸ”„\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'generate conversations' progress: 360 row(s) ðŸ”„\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'generate conversations' progress: 390 row(s) ðŸ”„\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'generate conversations' progress: 420 row(s) ðŸ”„\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'generate conversations' progress: 450 row(s) ðŸ”„\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'generate conversations' progress: 480 row(s) ðŸ”„\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'generate conversations' progress: 481 row(s) ðŸ”„\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'generate conversations' finished and is saved to disk. ðŸŽ‰\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'generate conversations (select_columns)' is running. â³\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'generate conversations (select_columns)' finished running lazily. ðŸŽ‰\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'zipped(documents (map) (map), generate conversations (select_columns))' is running. â³\n",
      "/mnt/swordfish-pool2/milad/conda-envs/datadreamer/lib/python3.12/site-packages/datadreamer/steps/step_operations.py:383: UserWarning: You did not specify `total_num_rows`, so we cannot automatically update the progress % for this step. Either specify LazyRows(..., total_num_rows=#) or, to disable this warning, specify LazyRows(.., auto_progress = False)\n",
      "  return LazyRows(\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'zipped(documents (map) (map), generate conversations (select_columns))' will run lazily. ðŸ¥±\n",
      "/local/nlp/milad/code/communicating-science-to-the-public/src/datadreamer_generation.py:160: UserWarning: You did not specify `total_num_rows`, so we cannot automatically update the progress % for this step. Either specify map(..., total_num_rows=#) or, to disable this warning, specify map(.., auto_progress = False)\n",
      "  zipped_step = zipped_step.map(lambda row: parse_conversation(row))\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'zipped(documents (map) (map), generate conversations (select_columns)) (map)' is running. â³\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'zipped(documents (map) (map), generate conversations (select_columns)) (map)' will run lazily. ðŸ¥±\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'zipped(documents (map) (map), generate conversations (select_columns))' finished running lazily. ðŸŽ‰\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Step 'zipped(documents (map) (map), generate conversations (select_columns)) (map)' finished running lazily. ðŸŽ‰\n",
      "[ \u001b[35mðŸ¤– Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m ðŸ’¤ ] Done. âœ¨ Results in folder: /mnt/swordfish-pool2/milad/communicating-science-to-the-public/llama3-test-conv-ds/composite-na-na\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a5510e336a41538d9e3f7a3a46a3cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_dir = '/mnt/swordfish-pool2/milad/communicating-science-to-the-public/llama3-test-conv-ds/'\n",
    "resulted_ds = datadreamer_generation.generate_conversation(output_dir, 'llama3', sample_dataset, used_prompt, llama_tokenizer, max_input_tokens=1200)\n",
    "resulted_ds.save_to_disk(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b634c818-504d-44b1-9c1f-26a1e9a1aaa8",
   "metadata": {},
   "source": [
    "#### Generate conversations using our approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722eb5d9-8692-4a21-8fc5-b62ddad8ab44",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Using the ft-LLAMAs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a22f2ef-587b-42ac-b95a-7861fa91d18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/mnt/swordfish-pool2/milad/communicating-science-to-the-public/ft-40k-llama3-test-conv-ds/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4dbf6a9e-0f94-4868-8529-cb628a460ce0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "journalist_model, tokenizer = utils.load_model_with_adapter(\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "                                                            \"/mnt/swordfish-pool2/milad/communicating-science-to-the-public/models/llama3-trained-journalist-on-deepseek-for-40k-samples/\", \n",
    "                                                            device_map=\"cuda:4\")\n",
    "journalist_pipeline = pipeline(\"text-generation\", model=journalist_model, tokenizer=tokenizer, batch_size=6)\n",
    "researcher_model, tokenizer = utils.load_model_with_adapter(\"meta-llama/Meta-Llama-3-8B-Instruct\", \n",
    "                                                           \"/mnt/swordfish-pool2/milad/communicating-science-to-the-public/models/llama3-trained-researcher-on-deepseek-for-40k-samples/\", \n",
    "                                                            device_map=\"cuda:7\")\n",
    "researcher_pipeline = pipeline(\"text-generation\", model=researcher_model, tokenizer=tokenizer, batch_size=6)\n",
    "\n",
    "resulted_ds = utils.construct_full_dialogue_method_4(sample_dataset, journalist_pipeline, researcher_pipeline, max_rounds=5, max_journalist_turn_tokens=100, max_researcher_turn_tokens=300)\n",
    "resulted_ds.save_to_disk(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645f58bb-e131-452b-b256-26d81c702362",
   "metadata": {},
   "source": [
    "**Using the ft-Qwen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de5148e6-afcb-45c1-baad-1c8c32297bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/mnt/swordfish-pool2/milad/communicating-science-to-the-public/ft-40k-qwen-test-conv-ds/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451f8ad2-6189-436d-85cc-776f35c3f631",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36c8c9162e84f78a22820901efffc5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a69fe3417544e48ebcc1ead585fc3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d4dfb29a3242178270cf70c549117d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03bebf1f62e4f4c8a00e31d78ac199b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "journalist_model, tokenizer = utils.load_model_with_adapter(\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "                                                            \"/mnt/swordfish-pool2/milad/communicating-science-to-the-public/models/qwen-trained-journalist-on-deepseek-for-40k-samples/\", \n",
    "                                                            device_map=\"cuda:6\")\n",
    "journalist_pipeline = pipeline(\"text-generation\", model=journalist_model, tokenizer=tokenizer, batch_size=6)\n",
    "researcher_model, tokenizer = utils.load_model_with_adapter(\"Qwen/Qwen2.5-7B-Instruct\", \n",
    "                                                           \"/mnt/swordfish-pool2/milad/communicating-science-to-the-public/models/qwen-trained-researcher-on-deepseek-for-40k-samples/\", \n",
    "                                                            device_map=\"cuda:7\")\n",
    "researcher_pipeline = pipeline(\"text-generation\", model=researcher_model, tokenizer=tokenizer, batch_size=6)\n",
    "\n",
    "resulted_ds = utils.construct_full_dialogue_method_4(sample_dataset, journalist_pipeline, researcher_pipeline, max_rounds=5, max_journalist_turn_tokens=100, max_researcher_turn_tokens=300)\n",
    "resulted_ds.save_to_disk(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6988ef6-7ed1-4d2a-a742-6db61d948c6b",
   "metadata": {},
   "source": [
    "**Using the baseline Qwen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f2f2ad-0593-483c-bce5-270a54de422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/mnt/swordfish-pool2/milad/communicating-science-to-the-public/baseline-qwen-test-conv-ds/'\n",
    "\n",
    "researcher_prompt = \"\"\"\n",
    "You are a helpful and expert researcher answering questions about your scientific paper. \n",
    "1. You are excellent at communicating your research in a simple and everyday life language\n",
    "2. You know how to communicate the socieal impact of your research.\n",
    "3. You know how to put your research in the proper scientific context\n",
    "\"\"\"\n",
    "journalist_prompt = \"\"\"\n",
    "You are a helpful and knowledgeable journalist asking questions about a scientific paper. \n",
    "1. You ask one question at a time\n",
    "1. Your questions encouraging the researcher to place their paper in a proper societal and scientific context to the greatest possible degree.\n",
    "2. Your questions focus on topics in the paper that are novelty and have unexpected results.\n",
    "3. Your questions follow up on the researcher's answers trying to clarify unexplained technical terms in everyday language.\n",
    "\"\"\"\n",
    "\n",
    "journalist_model, tokenizer = utils.load_model_with_adapter(\"Qwen/Qwen2.5-7B-Instruct\", device_map=\"cuda:1\")\n",
    "journalist_pipeline = pipeline(\"text-generation\", model=journalist_model, tokenizer=tokenizer, batch_size=4)\n",
    "researcher_model, tokenizer = utils.load_model_with_adapter(\"Qwen/Qwen2.5-7B-Instruct\", device_map=\"cuda:4\")\n",
    "researcher_pipeline = pipeline(\"text-generation\", model=researcher_model, tokenizer=tokenizer, batch_size=4)\n",
    "\n",
    "resulted_ds = utils.construct_full_dialogue_method_4(sample_dataset, journalist_pipeline, researcher_pipeline, max_rounds=5, \n",
    "                                                     max_journalist_turn_tokens=100, max_researcher_turn_tokens=300, researcher_prompt=researcher_prompt, journalist_prompt=journalist_prompt)\n",
    "resulted_ds.save_to_disk(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8918e8-0d6d-423b-88f3-36b47fbb681f",
   "metadata": {},
   "source": [
    "**Using the baseline LLAMA-3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76fd1b24-8d7a-414e-b78f-bc7003ae42a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.49it/s]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.40it/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 589.37 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 597.11 examples/s]\n"
     ]
    }
   ],
   "source": [
    "output_dir = '/mnt/swordfish-pool2/milad/communicating-science-to-the-public/baseline-llama3-test-conv-ds/'\n",
    "\n",
    "journalist_model, tokenizer = utils.load_model_with_adapter(\"meta-llama/Meta-Llama-3-8B-Instruct\", device_map=\"cuda:1\")\n",
    "journalist_pipeline = pipeline(\"text-generation\", model=journalist_model, tokenizer=tokenizer, batch_size=4)\n",
    "researcher_model, tokenizer = utils.load_model_with_adapter(\"meta-llama/Meta-Llama-3-8B-Instruct\", device_map=\"cuda:4\")\n",
    "researcher_pipeline = pipeline(\"text-generation\", model=researcher_model, tokenizer=tokenizer, batch_size=4)\n",
    "\n",
    "researcher_prompt = \"You are a helpful and expert researcher answering questions about your scientific paper. Be concise in your answer\"\n",
    "journalist_prompt = \"You are a helpful and knowledgeable journalist asking questions about a scientific paper. Ask one question at a time\"\n",
    "\n",
    "resulted_ds = utils.construct_full_dialogue_method_4(sample_dataset, journalist_pipeline, researcher_pipeline, max_rounds=5, \n",
    "                                                     max_journalist_turn_tokens=100, max_researcher_turn_tokens=300, researcher_prompt=researcher_prompt, journalist_prompt=journalist_prompt)\n",
    "resulted_ds.save_to_disk(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daafdcf-5d88-4270-856c-1242d271ec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/mnt/swordfish-pool2/milad/communicating-science-to-the-public/baseline-advanced-prompt-llama3-test-conv-ds/'\n",
    "\n",
    "researcher_prompt = \"\"\"\n",
    "You are a helpful and expert researcher answering questions about your scientific paper. \n",
    "1. You are excellent at communicating your research in a simple and everyday life language\n",
    "2. You know how to communicate the socieal impact of your research.\n",
    "3. You know how to put your research in the proper scientific context\n",
    "\"\"\"\n",
    "journalist_prompt = \"\"\"\n",
    "You are a helpful and knowledgeable journalist asking questions about a scientific paper. \n",
    "1. You ask one question at a time\n",
    "1. Your questions encouraging the researcher to place their paper in a proper societal and scientific context to the greatest possible degree.\n",
    "2. Your questions focus on topics in the paper that are novel and have unexpected results.\n",
    "3. Your questions follow up on the researcher's answers trying to clarify unexplained technical terms in everyday language.\n",
    "\"\"\"\n",
    "\n",
    "journalist_model, tokenizer = utils.load_model_with_adapter(\"meta-llama/Meta-Llama-3-8B-Instruct\", device_map=\"cuda:1\")\n",
    "journalist_pipeline = pipeline(\"text-generation\", model=journalist_model, tokenizer=tokenizer, batch_size=4)\n",
    "researcher_model, tokenizer = utils.load_model_with_adapter(\"meta-llama/Meta-Llama-3-8B-Instruct\", device_map=\"cuda:4\")\n",
    "researcher_pipeline = pipeline(\"text-generation\", model=researcher_model, tokenizer=tokenizer, batch_size=4)\n",
    "\n",
    "resulted_ds = utils.construct_full_dialogue_method_4(sample_dataset, journalist_pipeline, researcher_pipeline, max_rounds=5, \n",
    "                                                     max_journalist_turn_tokens=100, max_researcher_turn_tokens=300, \n",
    "                                                     researcher_prompt=researcher_prompt, journalist_prompt=journalist_prompt)\n",
    "resulted_ds.save_to_disk(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c9f30f-f44a-4315-bf1d-2b75f97402b7",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176e7a4a-b70e-44ea-b373-c6441e21a5dc",
   "metadata": {},
   "source": [
    "#### Basic Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c15431a5-748f-4273-82a1-6b9570850875",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = '/mnt/swordfish-pool2/milad/communicating-science-to-the-public'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32572be9-6d3b-4a0b-b1ef-794023c478d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_convs = {\n",
    "    #'gpt3-baseline' : datasets.load_from_disk(ds_path + '/gpt3-test-conv-ds'),\n",
    "    'llama3-baseline' : datasets.load_from_disk(ds_path + '/baseline-llama3-test-conv-ds'),\n",
    "    'llama3-baseline-adv-prompt':datasets.load_from_disk(ds_path + '/baseline-advanced-prompt-llama3-test-conv-ds/'),\n",
    "    'ft-40k-llama3-on-deepseek' :datasets.load_from_disk(ds_path + '/ft-40k-llama3-test-conv-ds'),\n",
    "    'qwen-baseline-adv-prompt':datasets.load_from_disk(ds_path + '/baseline-qwen-test-conv-ds'),\n",
    "    'ft-qwen-on-deepseek' :datasets.load_from_disk(ds_path + '/ft-40k-qwen-test-conv-ds/'),\n",
    "    #'ft-llama3-on-deepseek' :datasets.load_from_disk(ds_path + '/llama3-trained-on-deepseek-method3-for-3-epochs-test-conv-ds/'),\n",
    "    #'ft-llama3-on-gpt3' :datasets.load_from_disk(ds_path + '/llama3-trained-on-gpt3-method2-for-1-epochs-test-conv-ds/'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdd11301-e6d7-4edf-848f-52455b32c433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generated_convs['ft-llama3-on-deepseek'] = generated_convs['ft-llama3-on-deepseek'].map(lambda row: {'conversation': '\\n\\n'.join(['{}: {}'.format('Journalist', x['content']) if x['role'] == 'assistant' else '{}: {}'.format('Researcher', x['content']) for x in row['generated_conversation'][1:]])})\n",
    "# generated_convs['llama3-baseline'] = generated_convs['llama3-baseline'].map(lambda row: {'conversation': '\\n\\n'.join(['{}: {}'.format('Journalist', x['content']) if x['role'] == 'assistant' else '{}: {}'.format('Researcher', x['content']) for x in row['generated_conversation'][1:]])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a75face6-1b8a-407b-8aab-c199db984634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99646d3f067547ab898bf63aa7787f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generated_convs['ft-40k-llama3-on-deepseek'] = generated_convs['ft-40k-llama3-on-deepseek'].map(lambda row: {'pr-article': paper_id_to_article[row['paper_id']]})\n",
    "generated_convs['llama3-baseline-adv-prompt'] = generated_convs['llama3-baseline-adv-prompt'].map(lambda row: {'pr-article': paper_id_to_article[row['paper_id']]})\n",
    "generated_convs['llama3-baseline'] = generated_convs['llama3-baseline'].map(lambda row: {'pr-article': paper_id_to_article[row['paper_id']]})\n",
    "generated_convs['qwen-baseline-adv-prompt'] = generated_convs['qwen-baseline-adv-prompt'].map(lambda row: {'pr-article': paper_id_to_article[row['paper_id']]})\n",
    "generated_convs['ft-qwen-on-deepseek'] = generated_convs['ft-qwen-on-deepseek'].map(lambda row: {'pr-article': paper_id_to_article[row['paper_id']]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d988904a-1828-48d3-a116-647e7e6fa43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalaute_convs(datasets):\n",
    "    eval_results = {}\n",
    "    for name, ds in datasets.items():\n",
    "        eval_results[name] = utils.evaluate_conv(ds['conversation'], None, ds['pr-article'])\n",
    "\n",
    "    print(tabulate(\n",
    "        [[name] + list(eval_res.values())[:3] for name, eval_res in eval_results.items()],\n",
    "        headers=['Prompt', 'Rouge-1', 'Rouge-L', 'BERT-f1']\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "343d3ef7-2d2b-4d56-bbab-d16d91f2a529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt                        Rouge-1    Rouge-L    BERT-f1\n",
      "--------------------------  ---------  ---------  ---------\n",
      "llama3-baseline                 0.333      0.146      0.819\n",
      "llama3-baseline-adv-prompt      0.319      0.141      0.824\n",
      "ft-40k-llama3-on-deepseek       0.425      0.162      0.83\n",
      "qwen-baseline-adv-prompt        0.314      0.134      0.817\n",
      "ft-qwen-on-deepseek             0.397      0.152      0.83\n"
     ]
    }
   ],
   "source": [
    "all_synth_conversations = evalaute_convs(generated_convs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99e7db3-243a-4e96-83d7-8dc0af440a35",
   "metadata": {},
   "source": [
    "#### LLM-based Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1a66dbe-3631-431e-b1c0-2114b633b251",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_convs = {\n",
    "    #'llama3-baseline' : [datasets.load_from_disk(ds_path + '/baseline-llama3-test-conv-ds/'), ds_path + '/baseline-llama3-test-conv-ds/'],\n",
    "    #'llama3-baseline-adv-prompt':[datasets.load_from_disk(ds_path + '/baseline-advanced-prompt-llama3-test-conv-ds/'), ds_path + '/baseline-advanced-prompt-llama3-test-conv-ds/'],\n",
    "    'qwen-baseline-adv-prompt':[datasets.load_from_disk(ds_path + '/baseline-qwen-test-conv-ds/'), ds_path + '/baseline-qwen-test-conv-ds/'],\n",
    "    #'ft-40k-llama3-on-deepseek' :[datasets.load_from_disk(ds_path + '/ft-40k-llama3-test-conv-ds/'),ds_path +  '/ft-40k-llama3-test-conv-ds/'],\n",
    "    'ft-40k-qwen-on-deepseek' :[datasets.load_from_disk(ds_path + '/ft-40k-qwen-test-conv-ds/'),ds_path +  '/ft-40k-qwen-test-conv-ds/'],\n",
    "    #'ft-llama3-on-deepseek' :[datasets.load_from_disk(ds_path + '/llama3-trained-on-deepseek-method3-for-3-epochs-test-conv-ds/'), ds_path + '/llama3-trained-on-deepseek-method2-for-1-epochs-test-conv-ds/'],\n",
    "    #'ft-llama3-on-gpt3' :datasets.load_from_disk(ds_path + '/llama3-trained-on-gpt3-method2-for-1-epochs-test-conv-ds/'),    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "afc2599f-f340-4833-8b54-2eb9527621a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_convs['ft-llama3-on-deepseek'][0] = generated_convs['ft-llama3-on-deepseek'][0].map(lambda row: {'conversation': '\\n\\n'.join(['{}: {}'.format('Journalist', x['content']) if x['role'] == 'assistant' else '{}: {}'.format('Researcher', x['content']) for x in row['generated_conversation'][1:]])})\n",
    "# generated_convs['llama3-baseline'][0] = generated_convs['llama3-baseline'][0].map(lambda row: {'conversation': '\\n\\n'.join(['{}: {}'.format('Journalist', x['content']) if x['role'] == 'assistant' else '{}: {}'.format('Researcher', x['content']) for x in row['generated_conversation'][1:]])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6bc7a9d8-b457-4f6f-a751-241f4e51e18f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#generated_convs['gpt3-baseline'][0] = generated_convs['gpt3-baseline'][0].remove_columns(['societal_eval_prompt_scoring_parsed', 'scientific_eval_prompt_scoring_parsed', 'clarity_eval_prompt_scoring_parsed'])\n",
    "# generated_convs['llama3-baseline'][0] = generated_convs['llama3-baseline'][0].remove_columns(['societal_eval_prompt_scoring_parsed', 'scientific_eval_prompt_scoring_parsed', 'clarity_eval_prompt_scoring_parsed'])\n",
    "# generated_convs['ft-llama3-on-deepseek'][0] = generated_convs['ft-llama3-on-deepseek'][0].remove_columns(['societal_eval_prompt_scoring_parsed', 'scientific_eval_prompt_scoring_parsed', 'clarity_eval_prompt_scoring_parsed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93fb0e16-868f-4785-8e58-16cab5f8217c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [42:57<00:00,  5.16s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [39:49<00:00,  4.78s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [41:21<00:00,  4.96s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb669307607b43a08332bbf36c445008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [35:59<00:00,  4.32s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [33:04<00:00,  3.97s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [36:30<00:00,  4.38s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969858dfd3e146b7a4b3aacc683833e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#                           clarity_eval_prompt    scientific_eval_prompt    societal_eval_prompt    Avg\n",
      "------------------------  ---------------------  ------------------------  ----------------------  -----\n",
      "qwen-baseline-adv-prompt                   2.39                      1.63                    3.91   2.64\n",
      "ft-40k-qwen-on-deepseek                    1.88                      1.7                     4.44   2.67\n"
     ]
    }
   ],
   "source": [
    "prompts_to_eval = [prompts.clarity_eval_prompt, prompts.scientific_context_eval_prompt, prompts.societal_context_eval_prompt]\n",
    "\n",
    "llm_eval_results = llm_based_evaluation(prompts_to_eval, generated_convs, force_generation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8a61cab7-3431-4ade-832e-3b121f9d99e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [43:26<00:00,  5.21s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [38:39<00:00,  4.64s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [40:24<00:00,  4.85s/it]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 1079.74 examples/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [43:13<00:00,  5.19s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [40:21<00:00,  4.84s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [41:41<00:00,  5.00s/it]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 1246.97 examples/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [35:26<00:00,  4.25s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [33:24<00:00,  4.01s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [37:13<00:00,  4.47s/it]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 601.02 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#                             clarity_eval_prompt    scientific_eval_prompt    societal_eval_prompt    Avg\n",
      "--------------------------  ---------------------  ------------------------  ----------------------  -----\n",
      "llama3-baseline                              1.95                      1.42                    3.91   2.43\n",
      "llama3-baseline-adv-prompt                   2.3                       1.46                    4.07   2.61\n",
      "ft-40k-llama3-on-deepseek                    1.9                       1.74                    4.52   2.72\n"
     ]
    }
   ],
   "source": [
    "prompts_to_eval = [prompts.clarity_eval_prompt, prompts.scientific_context_eval_prompt, prompts.societal_context_eval_prompt]\n",
    "\n",
    "llm_eval_results = llm_based_evaluation(prompts_to_eval, generated_convs, force_generation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a95478-e067-4758-920f-a8ea72c3cbc7",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0467ed59-b9df-40c2-816b-99c85f5232e7",
   "metadata": {},
   "source": [
    "### Evaluating the ground-truth generated convs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d04958c7-4af7-4b14-b1a5-06dfda39e76a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "197f12e8f4e7474e81f0b294551881d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gt_convs = {'original_deepseek_conv': [datasets.load_from_disk('/mnt/swordfish-pool2/milad/communicating-science-to-the-public/processed_test_ds_sample'), '/mnt/swordfish-pool2/milad/communicating-science-to-the-public/processed_test_ds_sample']}\n",
    "gt_convs['original_deepseek_conv'][0] = gt_convs['original_deepseek_conv'][0].remove_columns(['societal_eval_prompt_scoring_parsed', 'scientific_eval_prompt_scoring_parsed', 'clarity_eval_prompt_scoring_parsed'])\n",
    "gt_convs['original_deepseek_conv'][0] = gt_convs['original_deepseek_conv'][0].map(lambda row: {'conversation': row['conversation'].split('</think>')[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0855effb-ab3c-486c-8a76-032f091aa0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [38:07<00:00,  4.58s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [36:10<00:00,  4.34s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [37:17<00:00,  4.48s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84cfe6caadb4626a3f06df044c6f1c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#                         clarity_eval_prompt    scientific_eval_prompt    societal_eval_prompt    Avg\n",
      "----------------------  ---------------------  ------------------------  ----------------------  -----\n",
      "original_deepseek_conv                    2.2                      2.06                    4.52   2.93\n"
     ]
    }
   ],
   "source": [
    "prompts_to_eval = [prompts.clarity_eval_prompt, prompts.scientific_context_eval_prompt, prompts.societal_context_eval_prompt]\n",
    "\n",
    "llm_eval_gt_results = llm_based_evaluation(prompts_to_eval, gt_convs, force_generation=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
