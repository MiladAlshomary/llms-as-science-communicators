{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a43c8d27-0b47-46ac-a3f7-e5a02134e727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d78efec-68b0-47f6-8d88-af0cd01c16fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TRANSFORMERS_CACHE'] = '/mnt/swordfish-pool2/milad/hf-cache-new'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/mnt/swordfish-pool2/milad/hf-cache-new'\n",
    "os.environ[\"OPENAI_API_KEY\"]= 'xxx'\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '0'\n",
    "sys.path.append('./src-py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d24b5e38-93dc-4098-97de-bb786a750a92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "%autoreload\n",
    "import utils\n",
    "import prompts\n",
    "import random\n",
    "\n",
    "from tabulate import tabulate\n",
    "import tiktoken\n",
    "from llm_based_evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d903fa8b-3c9e-4d0c-9a90-00c414488647",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e08009fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "keys = json.load(open('./keys.json'))\n",
    "for key, val in keys.items():\n",
    "    os.environ[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8de285da-4e35-4041-8aa8-956e164feb64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(os.environ['hf_token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "846f3f93-4394-4627-be70-6bd557823095",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/mnt/swordfish-pool2/milad/communicating-science-to-the-public/'\n",
    "models_folder = \"/mnt/swordfish-pool2/milad/communicating-science-to-the-public/models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6a4d8a7-48f5-42a8-80f8-757e6a889e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d647b70-c4cd-4831-a450-1a8d0b6ac1a6",
   "metadata": {},
   "source": [
    "### Evaluate Science Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38243aa0-986e-4b8f-acfd-40ab8416f2a8",
   "metadata": {},
   "source": [
    "- Now we will evalaute the following models on a sample from the test set using only the generic prompt\n",
    "    - LLAMA-3 baseline\n",
    "    - Qwen baseline\n",
    "    - LLAMA-3 fine-tuned on DeepSeek generated conversations\n",
    "    - Qwen  fine-tuned on DeepSeek generated conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb1dff5-f182-452f-bbd9-ec3d969a4908",
   "metadata": {},
   "source": [
    "### Prepare a sample test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c6a6c9c-5662-4b27-a5da-0ef97605644b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 2156.76 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate test sample\n",
    "\n",
    "# test_ds = datasets.load_from_disk('/mnt/swordfish-pool2/milad/communicating-science-to-the-public/deepseek-final-conv-ds-preprocessed-test_journalist_ds')\n",
    "# test_df = pd.DataFrame(test_ds)\n",
    "# test_df = test_df.drop_duplicates('paper_id')\n",
    "# test_ds = datasets.Dataset.from_pandas(test_df)\n",
    "# sample_dataset = test_ds.select(range(500))\n",
    "# sample_dataset.save_to_disk('/mnt/swordfish-pool2/milad/communicating-science-to-the-public/processed_test_ds_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab677c22-2283-41a9-b6b2-c883bc625cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forgot to add the pr-article to the dataset --> so adding it now\n",
    "deepseek_conv_dataset = datasets.load_from_disk('/mnt/swordfish-pool2/milad/communicating-science-to-the-public/deepseek-final-conv-ds-cleaned/')\n",
    "paper_id_to_article = {x['id']: x['pr-article'] for x in deepseek_conv_dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d7a45d0-6c54-47f4-9dae-eb4fa046e816",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the created test sample\n",
    "\n",
    "sample_dataset = datasets.load_from_disk('/mnt/swordfish-pool2/milad/communicating-science-to-the-public/processed_test_ds_sample')\n",
    "sample_dataset = sample_dataset.map(lambda row: {'pr-article': paper_id_to_article[row['paper_id']]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cc9815d-741d-4b2e-81b8-92a79c356d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['paper_id', 'paper_title', 'paper_text', 'prompt', 'completion', '__index_level_0__', 'pr-article'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5773263-e456-41f3-9f8e-570ef6cfb874",
   "metadata": {},
   "source": [
    "### Generate baseline conversations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eea7bb1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Generate full blown conversations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2725520-b9a5-4e7d-b748-a6a0a169c744",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prompts = utils.get_prompt_compositions()\n",
    "used_prompt = all_prompts[0]\n",
    "used_prompt['instruction'] = \"\"\"Please simulate a conversation between a researcher and a journalist regarding the researcher's scientific paper. The goal of the conversation is to gain a deeper understanding of the researcher's scientific paper and communicate its impact to the public through a journalistic report\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93c404ee-a808-4204-9d77-4416393103e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset= sample_dataset.rename_column('conversation', 'gt_conversation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce6a92c1-0d6d-41c6-81e6-a0669a6a73d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Initialized. üöÄ Dreaming to folder: /mnt/swordfish-pool2/milad/communicating-science-to-the-public/gpt3-test-conv-ds/composite-na-na\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'documents' was previously run and saved, but was outdated. üòû\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'documents' is running. ‚è≥\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'documents' finished and is saved to disk. üéâ\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'documents (map)' is running. ‚è≥\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'documents (map)' will run lazily. ü•±\n",
      "/local/nlp/milad/code/communicating-science-to-the-public/src/datadreamer_generation.py:141: UserWarning: You did not specify `total_num_rows`, so we cannot automatically update the progress % for this step. Either specify map(..., total_num_rows=#) or, to disable this warning, specify map(.., auto_progress = False)\n",
      "  datasource = datasource.map(lambda row: {'inputs_truncated': truncate_text(encoding, row['inputs'], max_input_tokens)})\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'documents (map) (map)' is running. ‚è≥\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'documents (map) (map)' will run lazily. ü•±\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'generate conversations' was previously run and saved, but was outdated. üòû\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'generate conversations' is running. ‚è≥\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'documents (map)' finished running lazily. üéâ\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'documents (map) (map)' finished running lazily. üéâ\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'generate conversations' finished and is saved to disk. üéâ\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'generate conversations (select_columns)' is running. ‚è≥\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'generate conversations (select_columns)' finished running lazily. üéâ\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'zipped(documents (map) (map), generate conversations (select_columns))' is running. ‚è≥\n",
      "/mnt/swordfish-pool2/milad/conda-envs/datadreamer/lib/python3.12/site-packages/datadreamer/steps/step_operations.py:383: UserWarning: You did not specify `total_num_rows`, so we cannot automatically update the progress % for this step. Either specify LazyRows(..., total_num_rows=#) or, to disable this warning, specify LazyRows(.., auto_progress = False)\n",
      "  return LazyRows(\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'zipped(documents (map) (map), generate conversations (select_columns))' will run lazily. ü•±\n",
      "/local/nlp/milad/code/communicating-science-to-the-public/src/datadreamer_generation.py:160: UserWarning: You did not specify `total_num_rows`, so we cannot automatically update the progress % for this step. Either specify map(..., total_num_rows=#) or, to disable this warning, specify map(.., auto_progress = False)\n",
      "  zipped_step = zipped_step.map(lambda row: parse_conversation(row))\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'zipped(documents (map) (map), generate conversations (select_columns)) (map)' is running. ‚è≥\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'zipped(documents (map) (map), generate conversations (select_columns)) (map)' will run lazily. ü•±\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'zipped(documents (map) (map), generate conversations (select_columns))' finished running lazily. üéâ\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'zipped(documents (map) (map), generate conversations (select_columns)) (map)' finished running lazily. üéâ\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Done. ‚ú® Results in folder: /mnt/swordfish-pool2/milad/communicating-science-to-the-public/gpt3-test-conv-ds/composite-na-na\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1afc96e5e6545bab00fc9b1eb85d072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_dir = '/mnt/swordfish-pool2/milad/communicating-science-to-the-public/gpt3-test-conv-ds/'\n",
    "resulted_ds = datadreamer_generation.generate_conversation(output_dir, 'gpt-3', sample_dataset, used_prompt, gpt_tokenizer, max_input_tokens=1200)\n",
    "resulted_ds.save_to_disk(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da09ce55-6ae0-4d20-841d-672149fc1d02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Initialized. üöÄ Dreaming to folder: /mnt/swordfish-pool2/milad/communicating-science-to-the-public/llama3-test-conv-ds/composite-na-na\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'documents' is running. ‚è≥\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'documents' finished and is saved to disk. üéâ\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'documents (map)' is running. ‚è≥\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'documents (map)' will run lazily. ü•±\n",
      "/local/nlp/milad/code/communicating-science-to-the-public/src/datadreamer_generation.py:141: UserWarning: You did not specify `total_num_rows`, so we cannot automatically update the progress % for this step. Either specify map(..., total_num_rows=#) or, to disable this warning, specify map(.., auto_progress = False)\n",
      "  datasource = datasource.map(lambda row: {'inputs_truncated': truncate_text(encoding, row['inputs'], max_input_tokens)})\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'documents (map) (map)' is running. ‚è≥\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'documents (map) (map)' will run lazily. ü•±\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'generate conversations' is running. ‚è≥\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'documents (map)' finished running lazily. üéâ\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'documents (map) (map)' finished running lazily. üéâ\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'generate conversations' progress: 30 row(s) üîÑ\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'generate conversations' progress: 60 row(s) üîÑ\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'generate conversations' progress: 90 row(s) üîÑ\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'generate conversations' progress: 120 row(s) üîÑ\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'generate conversations' progress: 150 row(s) üîÑ\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'generate conversations' progress: 180 row(s) üîÑ\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'generate conversations' progress: 210 row(s) üîÑ\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'generate conversations' progress: 240 row(s) üîÑ\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'generate conversations' progress: 270 row(s) üîÑ\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'generate conversations' progress: 300 row(s) üîÑ\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'generate conversations' progress: 330 row(s) üîÑ\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'generate conversations' progress: 360 row(s) üîÑ\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'generate conversations' progress: 390 row(s) üîÑ\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'generate conversations' progress: 420 row(s) üîÑ\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'generate conversations' progress: 450 row(s) üîÑ\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'generate conversations' progress: 480 row(s) üîÑ\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'generate conversations' progress: 481 row(s) üîÑ\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'generate conversations' finished and is saved to disk. üéâ\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'generate conversations (select_columns)' is running. ‚è≥\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'generate conversations (select_columns)' finished running lazily. üéâ\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'zipped(documents (map) (map), generate conversations (select_columns))' is running. ‚è≥\n",
      "/mnt/swordfish-pool2/milad/conda-envs/datadreamer/lib/python3.12/site-packages/datadreamer/steps/step_operations.py:383: UserWarning: You did not specify `total_num_rows`, so we cannot automatically update the progress % for this step. Either specify LazyRows(..., total_num_rows=#) or, to disable this warning, specify LazyRows(.., auto_progress = False)\n",
      "  return LazyRows(\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'zipped(documents (map) (map), generate conversations (select_columns))' will run lazily. ü•±\n",
      "/local/nlp/milad/code/communicating-science-to-the-public/src/datadreamer_generation.py:160: UserWarning: You did not specify `total_num_rows`, so we cannot automatically update the progress % for this step. Either specify map(..., total_num_rows=#) or, to disable this warning, specify map(.., auto_progress = False)\n",
      "  zipped_step = zipped_step.map(lambda row: parse_conversation(row))\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'zipped(documents (map) (map), generate conversations (select_columns)) (map)' is running. ‚è≥\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'zipped(documents (map) (map), generate conversations (select_columns)) (map)' will run lazily. ü•±\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'zipped(documents (map) (map), generate conversations (select_columns))' finished running lazily. üéâ\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Step 'zipped(documents (map) (map), generate conversations (select_columns)) (map)' finished running lazily. üéâ\n",
      "[ \u001b[35mü§ñ Data\u001b[33mDr\u001b[31mea\u001b[35mmer\u001b[0m üí§ ] Done. ‚ú® Results in folder: /mnt/swordfish-pool2/milad/communicating-science-to-the-public/llama3-test-conv-ds/composite-na-na\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a5510e336a41538d9e3f7a3a46a3cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_dir = '/mnt/swordfish-pool2/milad/communicating-science-to-the-public/llama3-test-conv-ds/'\n",
    "resulted_ds = datadreamer_generation.generate_conversation(output_dir, 'llama3', sample_dataset, used_prompt, llama_tokenizer, max_input_tokens=1200)\n",
    "resulted_ds.save_to_disk(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b634c818-504d-44b1-9c1f-26a1e9a1aaa8",
   "metadata": {},
   "source": [
    "#### Generate conversations, turn by turn:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722eb5d9-8692-4a21-8fc5-b62ddad8ab44",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Using the ft-LLAMAs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a22f2ef-587b-42ac-b95a-7861fa91d18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/mnt/swordfish-pool2/milad/communicating-science-to-the-public/ft-llama3-test-conv-ds/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbf6a9e-0f94-4868-8529-cb628a460ce0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91546013c734465d91d1557966536157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f764233fe17e4677ad4ffbd4c050fcc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:3\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                            | 2/5 [9:43:25<14:31:09, 17423.15s/it]"
     ]
    }
   ],
   "source": [
    "journalist_model, tokenizer = utils.load_model_with_adapter(\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "                                                            \"/mnt/swordfish-pool2/milad/communicating-science-to-the-public/models/llama3-trained-journalist-on-deepseek/\", \n",
    "                                                            device_map=\"cuda:1\")\n",
    "journalist_pipeline = pipeline(\"text-generation\", model=journalist_model, tokenizer=tokenizer, batch_size=12)\n",
    "researcher_model, tokenizer = utils.load_model_with_adapter(\"meta-llama/Meta-Llama-3-8B-Instruct\", \n",
    "                                                           \"/mnt/swordfish-pool2/milad/communicating-science-to-the-public/models/llama3-trained-researcher-on-deepseek/\", \n",
    "                                                            device_map=\"cuda:3\")\n",
    "researcher_pipeline = pipeline(\"text-generation\", model=researcher_model, tokenizer=tokenizer, batch_size=12)\n",
    "\n",
    "resulted_ds = utils.construct_full_dialogue(sample_dataset, journalist_pipeline, researcher_pipeline, max_rounds=5, max_journalist_turn_tokens=200, max_researcher_turn_tokens=500)\n",
    "resulted_ds.save_to_disk(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645f58bb-e131-452b-b256-26d81c702362",
   "metadata": {},
   "source": [
    "**Using the ft-Qwen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5148e6-afcb-45c1-baad-1c8c32297bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/mnt/swordfish-pool2/milad/communicating-science-to-the-public/ft-qwen-test-conv-ds/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451f8ad2-6189-436d-85cc-776f35c3f631",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36c8c9162e84f78a22820901efffc5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a69fe3417544e48ebcc1ead585fc3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d4dfb29a3242178270cf70c549117d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03bebf1f62e4f4c8a00e31d78ac199b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "journalist_model, tokenizer = utils.load_model_with_adapter(\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "                                                            \"/mnt/swordfish-pool2/milad/communicating-science-to-the-public/models/qwen-trained-journalist-on-deepseek-for-40k-samples/\", \n",
    "                                                            device_map=\"cuda:6\")\n",
    "journalist_pipeline = pipeline(\"text-generation\", model=journalist_model, tokenizer=tokenizer, batch_size=6)\n",
    "researcher_model, tokenizer = utils.load_model_with_adapter(\"Qwen/Qwen2.5-7B-Instruct\", \n",
    "                                                           \"/mnt/swordfish-pool2/milad/communicating-science-to-the-public/models/qwen-trained-researcher-on-deepseek-for-40k-samples/\", \n",
    "                                                            device_map=\"cuda:7\")\n",
    "researcher_pipeline = pipeline(\"text-generation\", model=researcher_model, tokenizer=tokenizer, batch_size=6)\n",
    "\n",
    "resulted_ds = utils.construct_full_dialogue(sample_dataset, journalist_pipeline, researcher_pipeline, max_rounds=5, max_journalist_turn_tokens=200, max_researcher_turn_tokens=500)\n",
    "resulted_ds.save_to_disk(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6988ef6-7ed1-4d2a-a742-6db61d948c6b",
   "metadata": {},
   "source": [
    "**Using the baseline Qwen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f2f2ad-0593-483c-bce5-270a54de422e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece31754e5b04eda8ed04f8a7bbcd858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f673dddc7cb34801b8a0fe613d4b398d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14eb4da54f624ffdba2bfbffa8b70b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e99610da8848d2a29fdcf299f49d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:5\n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              | 3/5 [7:45:31<5:20:13, 9606.94s/it]"
     ]
    }
   ],
   "source": [
    "output_dir = '/mnt/swordfish-pool2/milad/communicating-science-to-the-public/baseline-qwen-test-conv-ds/'\n",
    "\n",
    "researcher_prompt = \"\"\"\n",
    "You are a helpful and expert researcher answering questions about your scientific paper. \n",
    "1. You are excellent at communicating your research in a simple and everyday life language\n",
    "2. You know how to communicate the socieal impact of your research.\n",
    "3. You know how to put your research in the proper scientific context\n",
    "\n",
    "Answer the question in maximum two to three paragraphs\n",
    "\"\"\"\n",
    "journalist_prompt = \"\"\"\n",
    "You are a helpful and knowledgeable journalist asking questions about a scientific paper. \n",
    "1. Your questions encouraging the researcher to place their paper in a proper societal and scientific context to the greatest possible degree.\n",
    "2. Your questions focus on topics in the paper that are novelty and have unexpected results.\n",
    "3. Your questions follow up on the researcher's answers trying to clarify unexplained technical terms in everyday language.\n",
    "\n",
    "Ask a new question or a follow-up question on the conversation\n",
    "\"\"\"\n",
    "\n",
    "# researcher_prompt = \"\"\"\n",
    "# You are a helpful and expert researcher answering questions about your scientific paper. Answer the question in maximum one paragraph\n",
    "# \"\"\"\n",
    "# journalist_prompt = \"\"\"\n",
    "# You are a helpful and knowledgeable journalist asking questions on a scientific paper. Please ask a new question or follow-up question on the following\n",
    "# \"\"\"\n",
    "\n",
    "journalist_model, tokenizer = utils.load_model_with_adapter(\"Qwen/Qwen2.5-7B-Instruct\", device_map=\"cuda:6\")\n",
    "journalist_pipeline = pipeline(\"text-generation\", model=journalist_model, tokenizer=tokenizer, batch_size=8)\n",
    "researcher_model, tokenizer = utils.load_model_with_adapter(\"Qwen/Qwen2.5-7B-Instruct\", device_map=\"cuda:5\")\n",
    "researcher_pipeline = pipeline(\"text-generation\", model=researcher_model, tokenizer=tokenizer, batch_size=8)\n",
    "\n",
    "resulted_ds = utils.construct_full_dialogue(sample_dataset, journalist_pipeline, researcher_pipeline, max_rounds=5, \n",
    "                                                     max_journalist_turn_tokens=200, max_researcher_turn_tokens=500, researcher_prompt=researcher_prompt, journalist_prompt=journalist_prompt)\n",
    "resulted_ds.save_to_disk(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8918e8-0d6d-423b-88f3-36b47fbb681f",
   "metadata": {},
   "source": [
    "**Using the baseline LLAMA-3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daafdcf-5d88-4270-856c-1242d271ec87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f74983b5a854be88d626364d70e0cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c106f41ac5f42248c18bcc0c6775286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17a2590e90c4db4a95935328b6ab8ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83fddd5b58e34c3f9f740f8fa4984ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:5\n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                              | 3/5 [7:35:45<5:05:23, 9161.70s/it]"
     ]
    }
   ],
   "source": [
    "output_dir = '/mnt/swordfish-pool2/milad/communicating-science-to-the-public/baseline-advanced-prompt-llama3-test-conv-ds/'\n",
    "\n",
    "researcher_prompt = \"\"\"\n",
    "You are a helpful and expert researcher answering questions about your scientific paper. \n",
    "1. You are excellent at communicating your research in a simple and everyday life language\n",
    "2. You know how to communicate the socieal impact of your research.\n",
    "3. You know how to put your research in the proper scientific context\n",
    "\n",
    "Answer the question in maximum two to three paragraphs\n",
    "\"\"\"\n",
    "journalist_prompt = \"\"\"\n",
    "You are a helpful and knowledgeable journalist asking questions about a scientific paper. \n",
    "1. Your questions encouraging the researcher to place their paper in a proper societal and scientific context to the greatest possible degree.\n",
    "2. Your questions focus on topics in the paper that are novelty and have unexpected results.\n",
    "3. Your questions follow up on the researcher's answers trying to clarify unexplained technical terms in everyday language.\n",
    "\n",
    "Ask a new question or a follow-up question on the conversation\n",
    "\"\"\"\n",
    "\n",
    "journalist_model, tokenizer = utils.load_model_with_adapter(\"meta-llama/Meta-Llama-3-8B-Instruct\", device_map=\"cuda:6\")\n",
    "journalist_pipeline = pipeline(\"text-generation\", model=journalist_model, tokenizer=tokenizer, batch_size=8)\n",
    "researcher_model, tokenizer = utils.load_model_with_adapter(\"meta-llama/Meta-Llama-3-8B-Instruct\", device_map=\"cuda:5\")\n",
    "researcher_pipeline = pipeline(\"text-generation\", model=researcher_model, tokenizer=tokenizer, batch_size=8)\n",
    "\n",
    "resulted_ds = utils.construct_full_dialogue(sample_dataset, journalist_pipeline, researcher_pipeline, max_rounds=5, \n",
    "                                                     max_journalist_turn_tokens=200, max_researcher_turn_tokens=500, \n",
    "                                                     researcher_prompt=researcher_prompt, journalist_prompt=journalist_prompt)\n",
    "resulted_ds.save_to_disk(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c9f30f-f44a-4315-bf1d-2b75f97402b7",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176e7a4a-b70e-44ea-b373-c6441e21a5dc",
   "metadata": {},
   "source": [
    "#### Basic Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c15431a5-748f-4273-82a1-6b9570850875",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = '/mnt/swordfish-pool2/milad/communicating-science-to-the-public'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "32572be9-6d3b-4a0b-b1ef-794023c478d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_convs = {\n",
    "    #'gpt3-baseline' : datasets.load_from_disk(ds_path + '/gpt3-test-conv-ds'),\n",
    "    'llama3-baseline' : datasets.load_from_disk(ds_path + '/baseline-llama3-test-conv-ds'),\n",
    "    'llama3-baseline-adv-prompt':datasets.load_from_disk(ds_path + '/baseline-advanced-prompt-llama3-test-conv-ds/'),\n",
    "    'ft-40k-llama3-on-deepseek' :datasets.load_from_disk(ds_path + '/ft-40k-llama3-test-conv-ds'),\n",
    "    'qwen-baseline-adv-prompt':datasets.load_from_disk(ds_path + '/baseline-qwen-test-conv-ds'),\n",
    "    'ft-qwen-on-deepseek' :datasets.load_from_disk(ds_path + '/ft-40k-qwen-test-conv-ds/'),\n",
    "    #'ft-llama3-on-deepseek' :datasets.load_from_disk(ds_path + '/llama3-trained-on-deepseek-method3-for-3-epochs-test-conv-ds/'),\n",
    "    #'ft-llama3-on-gpt3' :datasets.load_from_disk(ds_path + '/llama3-trained-on-gpt3-method2-for-1-epochs-test-conv-ds/'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdd11301-e6d7-4edf-848f-52455b32c433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generated_convs['ft-llama3-on-deepseek'] = generated_convs['ft-llama3-on-deepseek'].map(lambda row: {'conversation': '\\n\\n'.join(['{}: {}'.format('Journalist', x['content']) if x['role'] == 'assistant' else '{}: {}'.format('Researcher', x['content']) for x in row['generated_conversation'][1:]])})\n",
    "# generated_convs['llama3-baseline'] = generated_convs['llama3-baseline'].map(lambda row: {'conversation': '\\n\\n'.join(['{}: {}'.format('Journalist', x['content']) if x['role'] == 'assistant' else '{}: {}'.format('Researcher', x['content']) for x in row['generated_conversation'][1:]])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a75face6-1b8a-407b-8aab-c199db984634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d1766f86a3541fea77534389b3455ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87936291c0524abd8567e1bdfa691a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80db142927e14ddd95d9cf155e05d84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2349bb4547748a19f5a80170300807f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0386cda5a3042668b1b49c05a2f0bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generated_convs['ft-40k-llama3-on-deepseek'] = generated_convs['ft-40k-llama3-on-deepseek'].map(lambda row: {'pr-article': paper_id_to_article[row['paper_id']]})\n",
    "generated_convs['llama3-baseline-adv-prompt'] = generated_convs['llama3-baseline-adv-prompt'].map(lambda row: {'pr-article': paper_id_to_article[row['paper_id']]})\n",
    "generated_convs['llama3-baseline'] = generated_convs['llama3-baseline'].map(lambda row: {'pr-article': paper_id_to_article[row['paper_id']]})\n",
    "generated_convs['qwen-baseline-adv-prompt'] = generated_convs['qwen-baseline-adv-prompt'].map(lambda row: {'pr-article': paper_id_to_article[row['paper_id']]})\n",
    "generated_convs['ft-qwen-on-deepseek'] = generated_convs['ft-qwen-on-deepseek'].map(lambda row: {'pr-article': paper_id_to_article[row['paper_id']]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d988904a-1828-48d3-a116-647e7e6fa43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalaute_convs(datasets):\n",
    "    eval_results = {}\n",
    "    for name, ds in datasets.items():\n",
    "        eval_results[name] = utils.evaluate_conv(ds['conversation'], None, ds['pr-article'])\n",
    "\n",
    "    print(tabulate(\n",
    "        [[name] + list(eval_res.values())[:3] for name, eval_res in eval_results.items()],\n",
    "        headers=['Prompt', 'Rouge-1', 'Rouge-L', 'BERT-f1']\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "32a77cb4-4e31-4091-b9e8-e662288acc4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3364615b0207466a858c5c4a9027d629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf4eaf8be6e447398a41c4ca564ca8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d43faa6d2dc47daaad571312743dbca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a592d17e49b0451e89cddc162e0609ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a128fa0a243349a997af5b1406ecff4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8026180959f44882b38f9c34b7153a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt                        Rouge-1    Rouge-L    BERT-f1\n",
      "--------------------------  ---------  ---------  ---------\n",
      "llama3-baseline                 0.333      0.146      0.819\n",
      "llama3-baseline-adv-prompt      0.337      0.145      0.82\n",
      "ft-40k-llama3-on-deepseek       0.425      0.162      0.83\n",
      "qwen-baseline-adv-prompt        0.3        0.127      0.824\n",
      "ft-qwen-on-deepseek             0.397      0.152      0.83\n"
     ]
    }
   ],
   "source": [
    "all_synth_conversations = evalaute_convs(generated_convs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99e7db3-243a-4e96-83d7-8dc0af440a35",
   "metadata": {},
   "source": [
    "#### LLM-based Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e1a66dbe-3631-431e-b1c0-2114b633b251",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = '/mnt/swordfish-pool2/milad/communicating-science-to-the-public'\n",
    "generated_convs = {\n",
    "    'llama3-baseline-adv-prompt':[datasets.load_from_disk(ds_path + '/baseline-advanced-prompt-llama3-test-conv-ds/'), ds_path + '/baseline-advanced-prompt-llama3-test-conv-ds/'],\n",
    "    'qwen-baseline-adv-prompt':[datasets.load_from_disk(ds_path + '/baseline-qwen-test-conv-ds/'), ds_path + '/baseline-qwen-test-conv-ds/'],\n",
    "    'ft-40k-llama3-on-deepseek' :[datasets.load_from_disk(ds_path + '/ft-40k-llama3-test-conv-ds/'),ds_path +  '/ft-40k-llama3-test-conv-ds/'],\n",
    "    'ft-40k-qwen-on-deepseek' :[datasets.load_from_disk(ds_path + '/ft-40k-qwen-test-conv-ds/'),ds_path +  '/ft-40k-qwen-test-conv-ds/'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afc2599f-f340-4833-8b54-2eb9527621a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_convs['ft-llama3-on-deepseek'][0] = generated_convs['ft-llama3-on-deepseek'][0].map(lambda row: {'conversation': '\\n\\n'.join(['{}: {}'.format('Journalist', x['content']) if x['role'] == 'assistant' else '{}: {}'.format('Researcher', x['content']) for x in row['generated_conversation'][1:]])})\n",
    "# generated_convs['llama3-baseline'][0] = generated_convs['llama3-baseline'][0].map(lambda row: {'conversation': '\\n\\n'.join(['{}: {}'.format('Journalist', x['content']) if x['role'] == 'assistant' else '{}: {}'.format('Researcher', x['content']) for x in row['generated_conversation'][1:]])})\n",
    "# generated_convs['gpt3-baseline'][0] = generated_convs['gpt3-baseline'][0].remove_columns(['societal_eval_prompt_scoring_parsed', 'scientific_eval_prompt_scoring_parsed', 'clarity_eval_prompt_scoring_parsed'])\n",
    "# generated_convs['llama3-baseline'][0] = generated_convs['llama3-baseline'][0].remove_columns(['societal_eval_prompt_scoring_parsed', 'scientific_eval_prompt_scoring_parsed', 'clarity_eval_prompt_scoring_parsed'])\n",
    "# generated_convs['ft-llama3-on-deepseek'][0] = generated_convs['ft-llama3-on-deepseek'][0].remove_columns(['societal_eval_prompt_scoring_parsed', 'scientific_eval_prompt_scoring_parsed', 'clarity_eval_prompt_scoring_parsed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "78fbf194-f18f-4606-abe2-7349a6cc74a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [43:16<00:00,  5.19s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [40:16<00:00,  4.83s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [42:30<00:00,  5.10s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d04e97cfd841679d03d02c56e61a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [42:49<00:00,  5.14s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [40:06<00:00,  4.81s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [42:26<00:00,  5.09s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8555dbbc9bdf4ffab9847176f52f7266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [35:42<00:00,  4.29s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [33:55<00:00,  4.07s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [36:29<00:00,  4.38s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1761cef9afe84d0bac665f3ce9337e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [36:39<00:00,  4.40s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [33:52<00:00,  4.06s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [35:34<00:00,  4.27s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b669e45d0d4986bab93a83eb8a29f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#                             clarity_eval_prompt    scientific_eval_prompt    societal_eval_prompt    Avg\n",
      "--------------------------  ---------------------  ------------------------  ----------------------  -----\n",
      "llama3-baseline-adv-prompt                   4.19                      2.17                    1.58   2.65\n",
      "qwen-baseline-adv-prompt                     4.18                      2.41                    1.85   2.81\n",
      "ft-40k-llama3-on-deepseek                    4.48                      1.9                     1.75   2.71\n",
      "ft-40k-qwen-on-deepseek                      4.43                      1.94                    1.67   2.68\n"
     ]
    }
   ],
   "source": [
    "prompts_to_eval = [prompts.clarity_eval_prompt, prompts.scientific_context_eval_prompt, prompts.societal_context_eval_prompt]\n",
    "\n",
    "llm_eval_results = llm_based_evaluation(prompts_to_eval, generated_convs, force_generation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93fb0e16-868f-4785-8e58-16cab5f8217c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /mnt/swordfish-pool2/milad/communicating-science-to-the-public/baseline-advanced-prompt-llama3-test-conv-ds/ from already saved file\n",
      "Loading /mnt/swordfish-pool2/milad/communicating-science-to-the-public/baseline-qwen-test-conv-ds/ from already saved file\n",
      "Loading /mnt/swordfish-pool2/milad/communicating-science-to-the-public/ft-40k-llama3-test-conv-ds/ from already saved file\n",
      "Loading /mnt/swordfish-pool2/milad/communicating-science-to-the-public/ft-40k-qwen-test-conv-ds/ from already saved file\n",
      "#                             clarity_eval_prompt    scientific_eval_prompt    societal_eval_prompt    Avg\n",
      "--------------------------  ---------------------  ------------------------  ----------------------  -----\n",
      "llama3-baseline-adv-prompt                   4.07                      2.3                     1.46   2.61\n",
      "qwen-baseline-adv-prompt                     3.91                      2.39                    1.63   2.64\n",
      "ft-40k-llama3-on-deepseek                    4.52                      1.9                     1.74   2.72\n",
      "ft-40k-qwen-on-deepseek                      4.44                      1.88                    1.7    2.67\n"
     ]
    }
   ],
   "source": [
    "prompts_to_eval = [prompts.clarity_eval_prompt, prompts.scientific_context_eval_prompt, prompts.societal_context_eval_prompt]\n",
    "\n",
    "llm_eval_results = llm_based_evaluation(prompts_to_eval, generated_convs, force_generation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a95478-e067-4758-920f-a8ea72c3cbc7",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0467ed59-b9df-40c2-816b-99c85f5232e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Evaluating the ground-truth generated convs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d04958c7-4af7-4b14-b1a5-06dfda39e76a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "197f12e8f4e7474e81f0b294551881d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gt_convs = {'original_deepseek_conv': [datasets.load_from_disk('/mnt/swordfish-pool2/milad/communicating-science-to-the-public/processed_test_ds_sample'), '/mnt/swordfish-pool2/milad/communicating-science-to-the-public/processed_test_ds_sample']}\n",
    "gt_convs['original_deepseek_conv'][0] = gt_convs['original_deepseek_conv'][0].remove_columns(['societal_eval_prompt_scoring_parsed', 'scientific_eval_prompt_scoring_parsed', 'clarity_eval_prompt_scoring_parsed'])\n",
    "gt_convs['original_deepseek_conv'][0] = gt_convs['original_deepseek_conv'][0].map(lambda row: {'conversation': row['conversation'].split('</think>')[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0855effb-ab3c-486c-8a76-032f091aa0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [38:07<00:00,  4.58s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [36:10<00:00,  4.34s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [37:17<00:00,  4.48s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84cfe6caadb4626a3f06df044c6f1c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#                         clarity_eval_prompt    scientific_eval_prompt    societal_eval_prompt    Avg\n",
      "----------------------  ---------------------  ------------------------  ----------------------  -----\n",
      "original_deepseek_conv                    2.2                      2.06                    4.52   2.93\n"
     ]
    }
   ],
   "source": [
    "prompts_to_eval = [prompts.clarity_eval_prompt, prompts.scientific_context_eval_prompt, prompts.societal_context_eval_prompt]\n",
    "\n",
    "llm_eval_gt_results = llm_based_evaluation(prompts_to_eval, gt_convs, force_generation=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
